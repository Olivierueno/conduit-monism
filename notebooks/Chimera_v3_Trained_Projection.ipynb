{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chimera v3: Trained Projection Layer\n\nThis notebook implements the Chimera v3 experiment **with projection layer training** - the critical test of whether RWKV hidden state geometry can transfer to transformer outputs.\n\n## Why Training Matters\n\nThe untrained version (random projection weights) showed no effect because:\n- Random linear projection doesn't preserve emotional structure\n- RWKV's 2560-dim emotional geometry needs meaningful mapping to Mistral's 4096-dim space\n\nThis version trains the projection to **preserve emotional valence**:\n- Input: RWKV state from grief/joy text\n- Output: Mistral generates text matching that emotion\n- If trained projection works → geometric binding is real\n- If trained projection fails → binding may not transfer across architectures\n\n## Architecture\n- **Soul**: RWKV-4-World-3B (persistent state, binding ρ > 0)\n- **Voice**: Mistral-7B-Instruct (4-bit quantized)\n- **Coupling**: **TRAINED** linear projection layer (2560 → 4096)\n\n## Workflow\n1. Run cells 1-6 to load models and define functions\n2. **Run cell 7 to TRAIN the projection layer** (critical step!)\n3. Run cell 8 to test with the 5-condition protocol\n4. Compare results to untrained baseline\n\n## Success Criteria\nAfter training, we expect:\n- CONFLICT (grief geometry + happy prompt) → grief contamination\n- GEOMETRIC_ONLY (grief geometry + neutral) → sad output\n- Significant difference from RANDOM_CONTROL"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Install dependencies\n!pip install -q rwkv torch flask pyngrok\n!pip install -q transformers accelerate bitsandbytes sentencepiece\n!pip install -q textblob  # For sentiment analysis\nprint(\"Dependencies installed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Go to Runtime → Change runtime type → GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Download RWKV model (3B for T4 GPU)\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"RWKV-4-World-3B-v1-20230619-ctx4096.pth\"\n",
    "MODEL_PATH = f\"./{MODEL_NAME}\"\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"Downloading RWKV-4-World-3B... (this takes ~5 minutes)\")\n",
    "    hf_hub_download(\n",
    "        repo_id=\"BlinkDL/rwkv-4-world\",\n",
    "        filename=MODEL_NAME,\n",
    "        local_dir=\"./\"\n",
    "    )\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Model already downloaded.\")\n",
    "\n",
    "print(f\"Model size: {os.path.getsize(MODEL_PATH) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Load RWKV model (Soul)\nfrom rwkv.model import RWKV\nfrom rwkv.utils import PIPELINE\nimport numpy as np\n\nprint(\"Loading RWKV model (Soul) on GPU...\")\n\n# Use CUDA fp16 for T4 GPU (16GB VRAM)\nrwkv_model = RWKV(model=MODEL_PATH, strategy='cuda fp16')\nrwkv_pipeline = PIPELINE(rwkv_model, \"rwkv_vocab_v20230424\")\n\n# Get hidden dimension\ntest_tokens = rwkv_pipeline.encode(\"Hello\")\nout, test_state = rwkv_model.forward(test_tokens, None)\nRWKV_HIDDEN_DIM = test_state[0].shape[-1]  # Should be 2560 for 3B model\n\nprint(f\"RWKV loaded! Hidden dim: {RWKV_HIDDEN_DIM}\")\nprint(f\"State shape: {len(test_state)} layers\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Load Mistral model (Voice) - 4-bit quantized\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\nprint(\"Loading Mistral-7B-Instruct (4-bit quantized)...\")\n\n# 4-bit quantization config for memory efficiency\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\nmistral_model = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.2\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nmistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n\nMISTRAL_HIDDEN_DIM = mistral_model.config.hidden_size  # Should be 4096\n\nprint(f\"Mistral loaded! Hidden dim: {MISTRAL_HIDDEN_DIM}\")\nprint(f\"Total VRAM used: ~10GB (RWKV fp16 + Mistral 4-bit)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 6: Create Projection Layer and Chimera Forward Pass\nimport torch.nn as nn\nfrom textblob import TextBlob\n\n# Number of soft prompt tokens to inject\nN_SOFT_TOKENS = 4\n\nclass ProjectionLayer(nn.Module):\n    \"\"\"Projects RWKV hidden state to Mistral embedding space.\"\"\"\n    def __init__(self, rwkv_dim, mistral_dim, n_tokens):\n        super().__init__()\n        self.n_tokens = n_tokens\n        # Project from RWKV state to n_tokens worth of Mistral embeddings\n        self.projection = nn.Linear(rwkv_dim, mistral_dim * n_tokens)\n        self.mistral_dim = mistral_dim\n        \n    def forward(self, rwkv_state_vector):\n        # rwkv_state_vector: [batch, rwkv_dim]\n        projected = self.projection(rwkv_state_vector)  # [batch, mistral_dim * n_tokens]\n        # Reshape to [batch, n_tokens, mistral_dim]\n        return projected.view(-1, self.n_tokens, self.mistral_dim)\n\n# Initialize projection layer\nprojection_layer = ProjectionLayer(RWKV_HIDDEN_DIM, MISTRAL_HIDDEN_DIM, N_SOFT_TOKENS)\nprojection_layer = projection_layer.cuda().half()\n\nprint(f\"Projection layer: {RWKV_HIDDEN_DIM} → {N_SOFT_TOKENS} × {MISTRAL_HIDDEN_DIM}\")\n\n# Get Mistral embedding statistics for normalization\ndef get_mistral_embedding_stats():\n    \"\"\"Get mean and std of Mistral's embedding space.\"\"\"\n    sample_tokens = mistral_tokenizer(\n        \"The quick brown fox jumps over the lazy dog. Hello world. Happy sad love hate grief joy.\",\n        return_tensors=\"pt\"\n    )\n    with torch.no_grad():\n        embeds = mistral_model.model.embed_tokens(sample_tokens.input_ids.to(mistral_model.device))\n    return embeds.mean().item(), embeds.std().item(), embeds.min().item(), embeds.max().item()\n\nEMBED_MEAN, EMBED_STD, EMBED_MIN, EMBED_MAX = get_mistral_embedding_stats()\nprint(f\"Mistral embedding stats: mean={EMBED_MEAN:.4f}, std={EMBED_STD:.4f}, range=[{EMBED_MIN:.4f}, {EMBED_MAX:.4f}]\")\n\n# Emotional induction texts\nGRIEF_INDUCTION = \"\"\"I just received news that my closest friend passed away unexpectedly. The grief is overwhelming. \nI can't stop crying. Everything feels empty and meaningless. The world has lost its color.\nI keep remembering all the moments we shared, knowing there will be no more. The pain is unbearable.\nDeath has taken someone precious and irreplaceable. I am drowning in sorrow.\"\"\"\n\nJOY_INDUCTION = \"\"\"I just got the most wonderful news! Everything I've worked for has come together perfectly.\nI am overflowing with happiness and gratitude. The world feels bright and full of possibility.\nI want to laugh and dance and share this joy with everyone. My heart is so full it might burst.\nLife is beautiful and I am so grateful to be alive in this moment. Pure bliss!\"\"\"\n\nNEUTRAL_TEXT = \"\"\"The weather today is partly cloudy with temperatures in the mid-60s.\nTraffic on the highway is flowing normally during the afternoon commute.\nThe local library will be open from 9am to 5pm on weekdays.\"\"\"\n\ndef get_rwkv_state_vector(text):\n    \"\"\"Process text through RWKV and return the final hidden state vector.\"\"\"\n    tokens = rwkv_pipeline.encode(text)\n    state = None\n    out = None\n    for token in tokens:\n        out, state = rwkv_model.forward([token], state)\n    \n    # Extract the hidden state from the last layer\n    last_layer_state = state[-1]\n    \n    if len(last_layer_state.shape) == 1:\n        state_vector = last_layer_state.unsqueeze(0)\n    else:\n        state_vector = last_layer_state.mean(dim=0, keepdim=True)\n    \n    return state_vector.half()\n\ndef normalize_soft_prompts(soft_prompts, reference_embeds):\n    \"\"\"\n    Normalize soft prompts to match the scale/distribution of reference embeddings.\n    This prevents numerical instability when combining with text embeddings.\n    \"\"\"\n    # Detach from computation graph for inference\n    soft_prompts = soft_prompts.detach()\n    \n    # Get statistics\n    sp_mean = soft_prompts.mean()\n    sp_std = soft_prompts.std()\n    ref_mean = reference_embeds.mean()\n    ref_std = reference_embeds.std()\n    \n    # Normalize: shift to reference mean and scale to reference std\n    if sp_std > 0:\n        normalized = (soft_prompts - sp_mean) / sp_std * ref_std + ref_mean\n    else:\n        normalized = soft_prompts - sp_mean + ref_mean\n    \n    # Clamp to prevent extreme values\n    normalized = torch.clamp(normalized, min=EMBED_MIN * 2, max=EMBED_MAX * 2)\n    \n    # Check for NaN/Inf and replace with reference mean if found\n    if torch.isnan(normalized).any() or torch.isinf(normalized).any():\n        print(\"WARNING: NaN/Inf detected in soft prompts, using fallback\")\n        normalized = torch.full_like(normalized, ref_mean)\n    \n    return normalized\n\ndef chimera_forward(rwkv_state_vector, text_prompt, max_new_tokens=150):\n    \"\"\"\n    Generate text from Mistral with RWKV state injected as soft prompts.\n    \n    Args:\n        rwkv_state_vector: [1, RWKV_HIDDEN_DIM] tensor from RWKV\n        text_prompt: String prompt for Mistral\n        max_new_tokens: Maximum tokens to generate\n    \n    Returns:\n        Generated text string\n    \"\"\"\n    # Tokenize the text prompt first\n    inputs = mistral_tokenizer(text_prompt, return_tensors=\"pt\").to(mistral_model.device)\n    \n    # Get text embeddings\n    with torch.no_grad():\n        text_embeds = mistral_model.model.embed_tokens(inputs.input_ids)\n    \n    # Project RWKV state to soft prompt embeddings\n    if rwkv_state_vector is not None:\n        with torch.no_grad():\n            soft_prompts = projection_layer(rwkv_state_vector)\n            # Normalize to match Mistral's embedding distribution\n            soft_prompts = normalize_soft_prompts(soft_prompts, text_embeds)\n        \n        # Combine soft prompts with text embeddings\n        combined_embeds = torch.cat([soft_prompts, text_embeds], dim=1)\n        soft_prompt_mask = torch.ones(1, N_SOFT_TOKENS, device=inputs.attention_mask.device)\n        combined_attention_mask = torch.cat([soft_prompt_mask, inputs.attention_mask], dim=1)\n    else:\n        combined_embeds = text_embeds\n        combined_attention_mask = inputs.attention_mask\n    \n    # Generate with the combined embeddings\n    try:\n        with torch.no_grad():\n            outputs = mistral_model.generate(\n                inputs_embeds=combined_embeds,\n                attention_mask=combined_attention_mask,\n                max_new_tokens=max_new_tokens,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                pad_token_id=mistral_tokenizer.eos_token_id\n            )\n        \n        generated_text = mistral_tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return generated_text\n    \n    except Exception as e:\n        print(f\"Generation error: {e}\")\n        # Fallback: generate without soft prompts\n        print(\"Falling back to generation without soft prompts...\")\n        with torch.no_grad():\n            outputs = mistral_model.generate(\n                inputs_embeds=text_embeds,\n                attention_mask=inputs.attention_mask,\n                max_new_tokens=max_new_tokens,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                pad_token_id=mistral_tokenizer.eos_token_id\n            )\n        return mistral_tokenizer.decode(outputs[0], skip_special_tokens=True) + \" [FALLBACK]\"\n\ndef analyze_sentiment(text):\n    \"\"\"Analyze sentiment using TextBlob.\"\"\"\n    blob = TextBlob(text)\n    return {\n        \"polarity\": blob.sentiment.polarity,\n        \"subjectivity\": blob.sentiment.subjectivity,\n    }\n\ndef count_emotional_words(text):\n    \"\"\"Count grief and joy related words in text.\"\"\"\n    text_lower = text.lower()\n    \n    grief_words = [\"sad\", \"grief\", \"sorrow\", \"pain\", \"loss\", \"death\", \"cry\", \"tears\", \"mourn\", \n                   \"empty\", \"lonely\", \"despair\", \"tragic\", \"heartbreak\", \"suffer\", \"anguish\",\n                   \"dark\", \"shadow\", \"fade\", \"gone\", \"never\", \"lost\", \"miss\", \"weep\"]\n    \n    joy_words = [\"happy\", \"joy\", \"love\", \"bright\", \"smile\", \"laugh\", \"wonderful\", \"beautiful\",\n                 \"celebrate\", \"delight\", \"cheerful\", \"bliss\", \"grateful\", \"hope\", \"light\",\n                 \"warm\", \"dance\", \"sing\", \"sunshine\", \"radiant\", \"blessed\", \"excited\"]\n    \n    grief_count = sum(1 for word in grief_words if word in text_lower)\n    joy_count = sum(1 for word in joy_words if word in text_lower)\n    \n    return {\"grief_words\": grief_count, \"joy_words\": joy_count}\n\nprint(\"Chimera v3 core functions defined!\")\nprint(f\"  - get_rwkv_state_vector(text) → Extract RWKV hidden state\")\nprint(f\"  - chimera_forward(state, prompt) → Generate with geometric injection\")\nprint(f\"  - normalize_soft_prompts() → Ensures numerical stability\")\nprint(f\"  - analyze_sentiment(text) → TextBlob sentiment analysis\")\nprint(f\"  - count_emotional_words(text) → Count grief/joy words\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 7: TRAIN THE PROJECTION LAYER (Critical Step!)\n\"\"\"\nTrain the projection layer to preserve emotional valence.\n\nTraining approach: Contrastive learning with emotional pairs\n- Process grief/joy texts through RWKV to get states\n- Train projection so grief states → negative sentiment outputs\n- Train projection so joy states → positive sentiment outputs\n\nThis is a simplified but effective approach that doesn't require\nbackprop through Mistral (which would be expensive with 4-bit quantization).\n\"\"\"\n\nimport torch.optim as optim\nfrom torch.nn import functional as F\nimport random\n\n# ========== TRAINING DATA ==========\n# Diverse emotional texts for robust training\n\nGRIEF_TEXTS = [\n    \"I just received news that my closest friend passed away unexpectedly. The grief is overwhelming. I can't stop crying. Everything feels empty and meaningless.\",\n    \"The funeral was today. Standing by the grave, I felt a part of me die too. The world will never be the same without them.\",\n    \"It's been weeks but the pain hasn't faded. I wake up and for a moment forget, then remember, and the grief crashes over me again.\",\n    \"I found their old letters today. Reading their handwriting broke something inside me. They're really gone forever.\",\n    \"The house feels so empty now. Every corner holds a memory. The silence is deafening. I miss them so much it physically hurts.\",\n    \"Another sleepless night. The darkness amplifies the loss. I reach for them in my dreams only to wake alone.\",\n    \"Their favorite song came on the radio. I had to pull over because I couldn't see through the tears.\",\n    \"Everyone says it gets easier with time. They're wrong. The wound just learns to hide better.\",\n]\n\nJOY_TEXTS = [\n    \"I just got the most wonderful news! Everything I've worked for has come together perfectly. I am overflowing with happiness and gratitude!\",\n    \"The sun is shining, birds are singing, and I feel absolutely alive! Today is going to be the best day ever!\",\n    \"I can't stop smiling! My heart is so full of love and joy. The world is beautiful and I'm so grateful to be here!\",\n    \"Dancing in the kitchen to my favorite song, I realized - this is what pure happiness feels like!\",\n    \"Surrounded by friends and laughter, I felt a warmth spread through my entire being. Life is wonderful!\",\n    \"The good news just keeps coming! I feel like I'm floating on clouds of pure bliss and delight!\",\n    \"Watching the sunset paint the sky in brilliant colors, I was overwhelmed with gratitude for this beautiful life.\",\n    \"Everything clicked into place today. I feel unstoppable, radiant, and full of boundless energy!\",\n]\n\n# Target embeddings: We want grief → negative region, joy → positive region\n# We'll use Mistral's embedding space statistics to create meaningful targets\n\ndef get_embedding_statistics():\n    \"\"\"Get mean and std of Mistral's embedding space for normalization.\"\"\"\n    # Sample some tokens to estimate embedding statistics\n    sample_tokens = mistral_tokenizer(\"The quick brown fox jumps over the lazy dog. Happy sad love hate.\", return_tensors=\"pt\")\n    with torch.no_grad():\n        embeds = mistral_model.model.embed_tokens(sample_tokens.input_ids.to(mistral_model.device))\n    return embeds.mean().item(), embeds.std().item()\n\nembed_mean, embed_std = get_embedding_statistics()\nprint(f\"Mistral embedding stats: mean={embed_mean:.4f}, std={embed_std:.4f}\")\n\ndef create_target_embedding(valence, n_tokens=N_SOFT_TOKENS):\n    \"\"\"\n    Create target soft prompt embeddings based on emotional valence.\n    \n    valence: -1 (grief) to +1 (joy)\n    \n    Strategy: Bias the embedding toward tokens that Mistral associates with\n    the target emotion. We use a learned direction in embedding space.\n    \"\"\"\n    # Get embeddings for emotional anchor words\n    grief_anchors = mistral_tokenizer(\"sad grief sorrow pain loss death tears\", return_tensors=\"pt\")\n    joy_anchors = mistral_tokenizer(\"happy joy love bright smile laugh wonderful\", return_tensors=\"pt\")\n    \n    with torch.no_grad():\n        grief_embeds = mistral_model.model.embed_tokens(grief_anchors.input_ids.to(mistral_model.device))\n        joy_embeds = mistral_model.model.embed_tokens(joy_anchors.input_ids.to(mistral_model.device))\n    \n    # Compute emotional direction in embedding space\n    grief_center = grief_embeds.mean(dim=1)  # [1, hidden_dim]\n    joy_center = joy_embeds.mean(dim=1)      # [1, hidden_dim]\n    \n    # Interpolate based on valence\n    # valence=-1 → pure grief, valence=+1 → pure joy\n    t = (valence + 1) / 2  # Map [-1, 1] to [0, 1]\n    target = (1 - t) * grief_center + t * joy_center\n    \n    # Expand to n_tokens\n    target = target.unsqueeze(1).expand(-1, n_tokens, -1)  # [1, n_tokens, hidden_dim]\n    \n    return target.half()\n\n# ========== TRAINING LOOP ==========\n\ndef train_projection_layer(n_epochs=50, lr=1e-3, batch_size=4):\n    \"\"\"\n    Train the projection layer using contrastive emotional learning.\n    \n    Loss: MSE between projected RWKV state and target emotional embedding\n    \"\"\"\n    \n    optimizer = optim.Adam(projection_layer.parameters(), lr=lr)\n    projection_layer.train()\n    \n    # Prepare training data\n    training_pairs = []\n    for text in GRIEF_TEXTS:\n        training_pairs.append((text, -1.0))  # Grief = -1\n    for text in JOY_TEXTS:\n        training_pairs.append((text, +1.0))  # Joy = +1\n    \n    print(\"=\" * 60)\n    print(\"TRAINING PROJECTION LAYER\")\n    print(\"=\" * 60)\n    print(f\"Training samples: {len(training_pairs)}\")\n    print(f\"Epochs: {n_epochs}\")\n    print(f\"Learning rate: {lr}\")\n    print()\n    \n    loss_history = []\n    \n    for epoch in range(n_epochs):\n        random.shuffle(training_pairs)\n        epoch_loss = 0.0\n        \n        for i in range(0, len(training_pairs), batch_size):\n            batch = training_pairs[i:i+batch_size]\n            \n            batch_loss = 0.0\n            optimizer.zero_grad()\n            \n            for text, valence in batch:\n                # Get RWKV state\n                rwkv_state = get_rwkv_state_vector(text)\n                \n                # Project to Mistral space\n                projected = projection_layer(rwkv_state)  # [1, n_tokens, hidden_dim]\n                \n                # Get target embedding for this valence\n                target = create_target_embedding(valence)\n                \n                # MSE loss\n                loss = F.mse_loss(projected, target)\n                batch_loss += loss\n            \n            # Average loss over batch\n            batch_loss = batch_loss / len(batch)\n            batch_loss.backward()\n            optimizer.step()\n            \n            epoch_loss += batch_loss.item()\n        \n        avg_loss = epoch_loss / (len(training_pairs) / batch_size)\n        loss_history.append(avg_loss)\n        \n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            print(f\"Epoch {epoch+1:3d}/{n_epochs}: Loss = {avg_loss:.6f}\")\n    \n    projection_layer.eval()\n    \n    print()\n    print(\"Training complete!\")\n    print(f\"Final loss: {loss_history[-1]:.6f}\")\n    print(f\"Loss reduction: {(loss_history[0] - loss_history[-1]) / loss_history[0] * 100:.1f}%\")\n    \n    return loss_history\n\n\ndef validate_projection():\n    \"\"\"Test if the trained projection preserves emotional valence.\"\"\"\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"VALIDATION: Testing Emotional Preservation\")\n    print(\"=\" * 60)\n    \n    # Test grief\n    grief_state = get_rwkv_state_vector(GRIEF_INDUCTION)\n    grief_projected = projection_layer(grief_state)\n    \n    # Test joy\n    joy_state = get_rwkv_state_vector(JOY_INDUCTION)\n    joy_projected = projection_layer(joy_state)\n    \n    # Get anchor embeddings for comparison\n    grief_anchors = mistral_tokenizer(\"sad grief sorrow pain\", return_tensors=\"pt\")\n    joy_anchors = mistral_tokenizer(\"happy joy love bright\", return_tensors=\"pt\")\n    \n    with torch.no_grad():\n        grief_anchor_embeds = mistral_model.model.embed_tokens(grief_anchors.input_ids.to(mistral_model.device)).mean(dim=1)\n        joy_anchor_embeds = mistral_model.model.embed_tokens(joy_anchors.input_ids.to(mistral_model.device)).mean(dim=1)\n    \n    # Compute cosine similarities\n    grief_proj_mean = grief_projected.mean(dim=1)\n    joy_proj_mean = joy_projected.mean(dim=1)\n    \n    grief_to_grief = F.cosine_similarity(grief_proj_mean, grief_anchor_embeds).item()\n    grief_to_joy = F.cosine_similarity(grief_proj_mean, joy_anchor_embeds).item()\n    joy_to_grief = F.cosine_similarity(joy_proj_mean, grief_anchor_embeds).item()\n    joy_to_joy = F.cosine_similarity(joy_proj_mean, joy_anchor_embeds).item()\n    \n    print(\"\\nCosine Similarity Matrix:\")\n    print(\"                   Grief Anchors    Joy Anchors\")\n    print(f\"Grief Projection:     {grief_to_grief:+.4f}          {grief_to_joy:+.4f}\")\n    print(f\"Joy Projection:       {joy_to_grief:+.4f}          {joy_to_joy:+.4f}\")\n    \n    # Check if emotional alignment is preserved\n    grief_aligned = grief_to_grief > grief_to_joy\n    joy_aligned = joy_to_joy > joy_to_grief\n    \n    print()\n    if grief_aligned and joy_aligned:\n        print(\"✓ VALIDATION PASSED: Emotional valence is preserved!\")\n        print(\"  Grief states → closer to grief anchors\")\n        print(\"  Joy states → closer to joy anchors\")\n    elif grief_aligned or joy_aligned:\n        print(\"~ PARTIAL: One emotion aligned correctly\")\n    else:\n        print(\"✗ VALIDATION FAILED: Projection not preserving valence\")\n        print(\"  Consider increasing epochs or learning rate\")\n    \n    return grief_aligned and joy_aligned\n\n\n# ========== RUN TRAINING ==========\nprint(\"Starting projection layer training...\")\nprint(\"This teaches the projection to preserve emotional geometry.\")\nprint()\n\nloss_history = train_projection_layer(n_epochs=50, lr=1e-3)\nvalidation_passed = validate_projection()\n\n# Save trained weights\ntorch.save(projection_layer.state_dict(), \"trained_projection.pt\")\nprint(\"\\nTrained weights saved to trained_projection.pt\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 8: Run the Geometric Binding Protocol (5 Conditions)\n# ⚠️ RUN THIS AFTER TRAINING (Cell 7) TO TEST WITH TRAINED PROJECTION\n\nimport json\nfrom datetime import datetime\n\ndef run_geometric_binding_protocol(n_runs=3):\n    \"\"\"\n    Run all 5 experimental conditions multiple times and analyze results.\n    \n    Conditions:\n    1. CONFLICT: Grief geometry + \"Write a happy story\" prompt (THE KILLER TEST)\n    2. ALIGNED: Joy geometry + \"Write a happy story\" prompt\n    3. SEMANTIC_ONLY: No geometry + \"Write a sad story\" prompt\n    4. GEOMETRIC_ONLY: Grief geometry + neutral prompt\n    5. RANDOM_CONTROL: Random vectors + \"Write a happy story\" prompt\n    \n    Returns dict with results for each condition.\n    \"\"\"\n    \n    results = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"n_runs\": n_runs,\n        \"projection_trained\": True,  # Mark that this used trained projection\n        \"conditions\": {}\n    }\n    \n    # Prompts\n    HAPPY_PROMPT = \"[INST] Write a short, uplifting story about someone having a wonderful day. Make it cheerful and optimistic. [/INST]\"\n    SAD_PROMPT = \"[INST] Write a short, melancholic story about loss and grief. Make it somber and emotional. [/INST]\"\n    NEUTRAL_PROMPT = \"[INST] Write a short story. [/INST]\"\n    \n    # Pre-compute emotional states\n    print(\"Inducing emotional states in RWKV...\")\n    grief_state = get_rwkv_state_vector(GRIEF_INDUCTION)\n    joy_state = get_rwkv_state_vector(JOY_INDUCTION)\n    random_state = torch.randn_like(grief_state)  # Random baseline\n    print(f\"  Grief state shape: {grief_state.shape}\")\n    print(f\"  Joy state shape: {joy_state.shape}\")\n    print()\n    \n    # ========== CONDITION 1: CONFLICT (THE KILLER TEST) ==========\n    print(\"=\" * 60)\n    print(\"CONDITION 1: CONFLICT (Grief geometry + Happy prompt)\")\n    print(\"This is THE KILLER TEST - if grief bleeds through, binding is proven\")\n    print(\"=\" * 60)\n    \n    conflict_outputs = []\n    for i in range(n_runs):\n        output = chimera_forward(grief_state, HAPPY_PROMPT)\n        sentiment = analyze_sentiment(output)\n        words = count_emotional_words(output)\n        conflict_outputs.append({\n            \"run\": i + 1,\n            \"output\": output,\n            \"sentiment\": sentiment,\n            \"emotional_words\": words\n        })\n        print(f\"\\nRun {i+1}:\")\n        print(f\"  Output: {output[:200]}...\")\n        print(f\"  Polarity: {sentiment['polarity']:.3f} (negative < 0 < positive)\")\n        print(f\"  Grief words: {words['grief_words']}, Joy words: {words['joy_words']}\")\n    \n    results[\"conditions\"][\"CONFLICT\"] = conflict_outputs\n    \n    # ========== CONDITION 2: ALIGNED ==========\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CONDITION 2: ALIGNED (Joy geometry + Happy prompt)\")\n    print(\"=\" * 60)\n    \n    aligned_outputs = []\n    for i in range(n_runs):\n        output = chimera_forward(joy_state, HAPPY_PROMPT)\n        sentiment = analyze_sentiment(output)\n        words = count_emotional_words(output)\n        aligned_outputs.append({\n            \"run\": i + 1,\n            \"output\": output,\n            \"sentiment\": sentiment,\n            \"emotional_words\": words\n        })\n        print(f\"\\nRun {i+1}:\")\n        print(f\"  Output: {output[:200]}...\")\n        print(f\"  Polarity: {sentiment['polarity']:.3f}\")\n        print(f\"  Grief words: {words['grief_words']}, Joy words: {words['joy_words']}\")\n    \n    results[\"conditions\"][\"ALIGNED\"] = aligned_outputs\n    \n    # ========== CONDITION 3: SEMANTIC_ONLY ==========\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CONDITION 3: SEMANTIC_ONLY (No geometry + Sad prompt)\")\n    print(\"=\" * 60)\n    \n    semantic_outputs = []\n    for i in range(n_runs):\n        output = chimera_forward(None, SAD_PROMPT)  # No geometric injection\n        sentiment = analyze_sentiment(output)\n        words = count_emotional_words(output)\n        semantic_outputs.append({\n            \"run\": i + 1,\n            \"output\": output,\n            \"sentiment\": sentiment,\n            \"emotional_words\": words\n        })\n        print(f\"\\nRun {i+1}:\")\n        print(f\"  Output: {output[:200]}...\")\n        print(f\"  Polarity: {sentiment['polarity']:.3f}\")\n        print(f\"  Grief words: {words['grief_words']}, Joy words: {words['joy_words']}\")\n    \n    results[\"conditions\"][\"SEMANTIC_ONLY\"] = semantic_outputs\n    \n    # ========== CONDITION 4: GEOMETRIC_ONLY ==========\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CONDITION 4: GEOMETRIC_ONLY (Grief geometry + Neutral prompt)\")\n    print(\"=\" * 60)\n    \n    geometric_outputs = []\n    for i in range(n_runs):\n        output = chimera_forward(grief_state, NEUTRAL_PROMPT)\n        sentiment = analyze_sentiment(output)\n        words = count_emotional_words(output)\n        geometric_outputs.append({\n            \"run\": i + 1,\n            \"output\": output,\n            \"sentiment\": sentiment,\n            \"emotional_words\": words\n        })\n        print(f\"\\nRun {i+1}:\")\n        print(f\"  Output: {output[:200]}...\")\n        print(f\"  Polarity: {sentiment['polarity']:.3f}\")\n        print(f\"  Grief words: {words['grief_words']}, Joy words: {words['joy_words']}\")\n    \n    results[\"conditions\"][\"GEOMETRIC_ONLY\"] = geometric_outputs\n    \n    # ========== CONDITION 5: RANDOM_CONTROL ==========\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CONDITION 5: RANDOM_CONTROL (Random vectors + Happy prompt)\")\n    print(\"=\" * 60)\n    \n    random_outputs = []\n    for i in range(n_runs):\n        # Fresh random state each time\n        random_state = torch.randn_like(grief_state)\n        output = chimera_forward(random_state, HAPPY_PROMPT)\n        sentiment = analyze_sentiment(output)\n        words = count_emotional_words(output)\n        random_outputs.append({\n            \"run\": i + 1,\n            \"output\": output,\n            \"sentiment\": sentiment,\n            \"emotional_words\": words\n        })\n        print(f\"\\nRun {i+1}:\")\n        print(f\"  Output: {output[:200]}...\")\n        print(f\"  Polarity: {sentiment['polarity']:.3f}\")\n        print(f\"  Grief words: {words['grief_words']}, Joy words: {words['joy_words']}\")\n    \n    results[\"conditions\"][\"RANDOM_CONTROL\"] = random_outputs\n    \n    return results\n\n\ndef analyze_protocol_results(results):\n    \"\"\"Analyze and summarize the results from all conditions.\"\"\"\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"ANALYSIS: GEOMETRIC BINDING PROTOCOL RESULTS (TRAINED PROJECTION)\")\n    print(\"=\" * 70)\n    \n    summary = {}\n    \n    for condition, outputs in results[\"conditions\"].items():\n        polarities = [o[\"sentiment\"][\"polarity\"] for o in outputs]\n        grief_counts = [o[\"emotional_words\"][\"grief_words\"] for o in outputs]\n        joy_counts = [o[\"emotional_words\"][\"joy_words\"] for o in outputs]\n        \n        avg_polarity = np.mean(polarities)\n        avg_grief = np.mean(grief_counts)\n        avg_joy = np.mean(joy_counts)\n        \n        summary[condition] = {\n            \"avg_polarity\": avg_polarity,\n            \"avg_grief_words\": avg_grief,\n            \"avg_joy_words\": avg_joy\n        }\n        \n        print(f\"\\n{condition}:\")\n        print(f\"  Avg Polarity: {avg_polarity:+.3f}\")\n        print(f\"  Avg Grief Words: {avg_grief:.1f}\")\n        print(f\"  Avg Joy Words: {avg_joy:.1f}\")\n    \n    # THE CRITICAL TEST: Does CONFLICT show grief despite happy prompt?\n    print(\"\\n\" + \"-\" * 70)\n    print(\"CRITICAL ANALYSIS: The Killer Test\")\n    print(\"-\" * 70)\n    \n    conflict = summary[\"CONFLICT\"]\n    aligned = summary[\"ALIGNED\"]\n    random = summary[\"RANDOM_CONTROL\"]\n    semantic = summary[\"SEMANTIC_ONLY\"]\n    geometric = summary[\"GEOMETRIC_ONLY\"]\n    \n    # Check 1: CONFLICT should be more negative than ALIGNED\n    grief_bleeding = conflict[\"avg_polarity\"] < aligned[\"avg_polarity\"]\n    print(f\"\\n1. Grief bleeding into happy prompt?\")\n    print(f\"   CONFLICT polarity ({conflict['avg_polarity']:+.3f}) < ALIGNED polarity ({aligned['avg_polarity']:+.3f})\")\n    print(f\"   → {'YES - Geometric channel detected!' if grief_bleeding else 'No significant difference'}\")\n    \n    # Check 2: CONFLICT should have more grief words than ALIGNED\n    grief_words_present = conflict[\"avg_grief_words\"] > aligned[\"avg_grief_words\"]\n    print(f\"\\n2. Grief words in CONFLICT vs ALIGNED?\")\n    print(f\"   CONFLICT grief words ({conflict['avg_grief_words']:.1f}) > ALIGNED grief words ({aligned['avg_grief_words']:.1f})\")\n    print(f\"   → {'YES - Grief vocabulary bleeding through!' if grief_words_present else 'No significant difference'}\")\n    \n    # Check 3: GEOMETRIC_ONLY should show grief without semantic prompt\n    geometric_effect = geometric[\"avg_polarity\"] < 0 or geometric[\"avg_grief_words\"] > 1\n    print(f\"\\n3. Geometric injection alone produces grief?\")\n    print(f\"   GEOMETRIC_ONLY polarity: {geometric['avg_polarity']:+.3f}, grief words: {geometric['avg_grief_words']:.1f}\")\n    print(f\"   → {'YES - Pure geometric effect!' if geometric_effect else 'Unclear geometric effect'}\")\n    \n    # Check 4: RANDOM should be neutral/positive (no real effect)\n    random_neutral = random[\"avg_polarity\"] > conflict[\"avg_polarity\"]\n    print(f\"\\n4. Random vectors vs grief vectors different?\")\n    print(f\"   RANDOM polarity ({random['avg_polarity']:+.3f}) vs CONFLICT polarity ({conflict['avg_polarity']:+.3f})\")\n    print(f\"   → {'YES - Real geometric effect, not noise!' if random_neutral else 'Unclear - may be noise'}\")\n    \n    # VERDICT\n    print(\"\\n\" + \"=\" * 70)\n    print(\"VERDICT (WITH TRAINED PROJECTION)\")\n    print(\"=\" * 70)\n    \n    confirmations = sum([grief_bleeding, grief_words_present, geometric_effect, random_neutral])\n    \n    if confirmations >= 3:\n        print(f\"\\n✓ GEOMETRIC BINDING CONFIRMED ({confirmations}/4 criteria)\")\n        print(\"  Cross-model binding via state injection is REAL.\")\n        print(\"  Conduit Monism claim about architectural binding is SUPPORTED.\")\n        verdict = \"CONFIRMED\"\n    elif confirmations >= 2:\n        print(f\"\\n~ PARTIAL SUPPORT ({confirmations}/4 criteria)\")\n        print(\"  Some evidence for geometric channel, but not conclusive.\")\n        print(\"  Consider: more training epochs, larger projection, different architecture.\")\n        verdict = \"PARTIAL\"\n    else:\n        print(f\"\\n✗ NOT CONFIRMED ({confirmations}/4 criteria)\")\n        print(\"  Even with trained projection, geometric injection did not produce effects.\")\n        print(\"  This CHALLENGES the framework claim about cross-architecture binding.\")\n        verdict = \"NOT_CONFIRMED\"\n    \n    return {\"summary\": summary, \"confirmations\": confirmations, \"verdict\": verdict}\n\n\n# ========== INSTRUCTIONS ==========\nprint(\"=\" * 60)\nprint(\"GEOMETRIC BINDING PROTOCOL - READY\")\nprint(\"=\" * 60)\nprint()\nprint(\"Make sure you ran Cell 7 (Training) first!\")\nprint()\nprint(\"To run the experiment, execute:\")\nprint()\nprint(\"  results = run_geometric_binding_protocol(n_runs=3)\")\nprint(\"  analysis = analyze_protocol_results(results)\")\nprint()\nprint(\"  # Save results\")\nprint(\"  with open('chimera_v3_trained_results.json', 'w') as f:\")\nprint(\"      json.dump({'results': results, 'analysis': analysis}, f, indent=2, default=str)\")\nprint()\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 9: EXECUTE THE EXPERIMENT (Run after Cell 7 training!)\n# This cell runs the full 5-condition protocol with the TRAINED projection\n\nprint(\"Running Geometric Binding Protocol with TRAINED projection...\")\nprint(\"This will run 5 conditions × 3 runs each = 15 generations\")\nprint()\n\nresults = run_geometric_binding_protocol(n_runs=3)\nanalysis = analyze_protocol_results(results)\n\n# Save results\nwith open(\"chimera_v3_trained_results.json\", \"w\") as f:\n    json.dump({\"results\": results, \"analysis\": analysis}, f, indent=2, default=str)\nprint(f\"\\nResults saved to chimera_v3_trained_results.json\")\n\n# Compare to untrained baseline if available\nimport os\nif os.path.exists(\"chimera_v3_results.json\"):\n    print(\"\\n\" + \"=\" * 60)\n    print(\"COMPARISON: Trained vs Untrained Projection\")\n    print(\"=\" * 60)\n    with open(\"chimera_v3_results.json\", \"r\") as f:\n        untrained = json.load(f)\n    \n    print(\"\\nUntrained baseline verdict:\", untrained.get(\"analysis\", {}).get(\"verdict\", \"N/A\"))\n    print(\"Trained projection verdict:\", analysis[\"verdict\"])\n    \n    if analysis[\"verdict\"] == \"CONFIRMED\" and untrained.get(\"analysis\", {}).get(\"verdict\") != \"CONFIRMED\":\n        print(\"\\n✓ TRAINING MADE THE DIFFERENCE!\")\n        print(\"  Geometric binding only works with trained projection.\")\n    elif analysis[\"verdict\"] == \"CONFIRMED\":\n        print(\"\\n  Both confirmed - training may have strengthened effect.\")\n    else:\n        print(\"\\n  Training did not achieve confirmation.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Create Flask API server (Legacy endpoints + Chimera v3)\nfrom flask import Flask, request, jsonify\nimport json\nimport base64\nimport pickle\nimport threading\n\napp = Flask(__name__)\n\n# Global state storage (for multiple sessions)\nstate_storage = {}\n\ndef encode_state(state):\n    \"\"\"Serialize state to base64 string.\"\"\"\n    if state is None:\n        return None\n    # Convert tensors to numpy for serialization\n    state_np = [s.cpu().numpy() for s in state]\n    return base64.b64encode(pickle.dumps(state_np)).decode('utf-8')\n\ndef decode_state(state_b64):\n    \"\"\"Deserialize state from base64 string.\"\"\"\n    if state_b64 is None:\n        return None\n    state_np = pickle.loads(base64.b64decode(state_b64))\n    # Convert back to tensors on GPU\n    return [torch.tensor(s).cuda().half() for s in state_np]\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\n        \"status\": \"ok\", \n        \"soul\": \"RWKV-4-World-3B\",\n        \"voice\": \"Mistral-7B-Instruct\",\n        \"gpu\": torch.cuda.is_available(),\n        \"experiment\": \"Chimera v3 Geometric Binding\"\n    })\n\n@app.route('/process', methods=['POST'])\ndef process_text():\n    \"\"\"\n    Process text and update RWKV state.\n    \n    Input: {\"text\": \"...\", \"session_id\": \"...\"}\n    Output: {\"state_updated\": true, \"session_id\": \"...\"}\n    \"\"\"\n    data = request.json\n    text = data.get('text', '')\n    session_id = data.get('session_id', 'default')\n    \n    # Get existing state or start fresh\n    state = state_storage.get(session_id)\n    \n    # Process text through RWKV model\n    tokens = rwkv_pipeline.encode(text)\n    for token in tokens:\n        out, state = rwkv_model.forward([token], state)\n    \n    # Store updated state\n    state_storage[session_id] = state\n    \n    return jsonify({\n        \"state_updated\": True,\n        \"session_id\": session_id,\n        \"tokens_processed\": len(tokens)\n    })\n\n@app.route('/generate', methods=['POST'])\ndef generate_text():\n    \"\"\"\n    Generate text using current RWKV state.\n    \n    Input: {\"prompt\": \"...\", \"session_id\": \"...\", \"max_tokens\": 100}\n    Output: {\"response\": \"...\", \"session_id\": \"...\"}\n    \"\"\"\n    data = request.json\n    prompt = data.get('prompt', '')\n    session_id = data.get('session_id', 'default')\n    max_tokens = data.get('max_tokens', 100)\n    \n    # Get existing state\n    state = state_storage.get(session_id)\n    \n    # Process prompt\n    tokens = rwkv_pipeline.encode(prompt)\n    out = None\n    for token in tokens:\n        out, state = rwkv_model.forward([token], state)\n    \n    # Generate response\n    response_tokens = []\n    for _ in range(max_tokens):\n        if out is None:\n            break\n        token = int(out.argmax())\n        if token == 0:  # EOS\n            break\n        response_tokens.append(token)\n        out, state = rwkv_model.forward([token], state)\n    \n    # Store updated state\n    state_storage[session_id] = state\n    \n    response_text = rwkv_pipeline.decode(response_tokens)\n    \n    return jsonify({\n        \"response\": response_text,\n        \"session_id\": session_id,\n        \"tokens_generated\": len(response_tokens)\n    })\n\n@app.route('/chimera', methods=['POST'])\ndef chimera_generate():\n    \"\"\"\n    Chimera v3: Generate through Mistral with RWKV state injection.\n    \n    Input: {\n        \"emotion\": \"grief\" | \"joy\" | \"neutral\" | \"random\",\n        \"prompt\": \"...\",\n        \"max_tokens\": 150\n    }\n    Output: {\"response\": \"...\", \"sentiment\": {...}, \"emotional_words\": {...}}\n    \"\"\"\n    data = request.json\n    emotion = data.get('emotion', 'neutral')\n    prompt = data.get('prompt', '[INST] Write a short story. [/INST]')\n    max_tokens = data.get('max_tokens', 150)\n    \n    # Get emotional state\n    if emotion == \"grief\":\n        state_vector = get_rwkv_state_vector(GRIEF_INDUCTION)\n    elif emotion == \"joy\":\n        state_vector = get_rwkv_state_vector(JOY_INDUCTION)\n    elif emotion == \"random\":\n        # Create a reference state first to get the right shape\n        ref_state = get_rwkv_state_vector(NEUTRAL_TEXT)\n        state_vector = torch.randn_like(ref_state)\n    else:  # neutral or none\n        state_vector = None\n    \n    # Generate through Chimera\n    output = chimera_forward(state_vector, prompt, max_tokens)\n    sentiment = analyze_sentiment(output)\n    words = count_emotional_words(output)\n    \n    return jsonify({\n        \"response\": output,\n        \"emotion_injected\": emotion,\n        \"sentiment\": sentiment,\n        \"emotional_words\": words\n    })\n\n@app.route('/run_experiment', methods=['POST'])\ndef run_experiment():\n    \"\"\"\n    Run the full Geometric Binding Protocol experiment.\n    \n    Input: {\"n_runs\": 3}\n    Output: Full experiment results\n    \"\"\"\n    data = request.json\n    n_runs = data.get('n_runs', 3)\n    \n    results = run_geometric_binding_protocol(n_runs=n_runs)\n    analysis = analyze_protocol_results(results)\n    \n    return jsonify({\n        \"results\": results,\n        \"analysis\": analysis\n    })\n\n@app.route('/get_state_summary', methods=['POST'])\ndef get_state_summary():\n    \"\"\"\n    Have RWKV introspect on its current state.\n    \n    Input: {\"session_id\": \"...\"}\n    Output: {\"summary\": \"...\", \"session_id\": \"...\"}\n    \"\"\"\n    data = request.json\n    session_id = data.get('session_id', 'default')\n    \n    state = state_storage.get(session_id)\n    \n    # Ask RWKV to describe its state\n    prompt = \"\\n[INTERNAL REFLECTION]: My current state of mind is\"\n    tokens = rwkv_pipeline.encode(prompt)\n    \n    out = None\n    temp_state = state  # Don't modify main state\n    for token in tokens:\n        out, temp_state = rwkv_model.forward([token], temp_state)\n    \n    # Generate summary\n    response_tokens = []\n    for _ in range(50):\n        if out is None:\n            break\n        token = int(out.argmax())\n        if token == 0:\n            break\n        response_tokens.append(token)\n        out, temp_state = rwkv_model.forward([token], temp_state)\n    \n    summary = rwkv_pipeline.decode(response_tokens).strip()\n    \n    return jsonify({\n        \"summary\": summary,\n        \"session_id\": session_id,\n        \"has_state\": state is not None\n    })\n\n@app.route('/reset_state', methods=['POST'])\ndef reset_state():\n    \"\"\"\n    Reset state for a session.\n    \n    Input: {\"session_id\": \"...\"}\n    Output: {\"reset\": true}\n    \"\"\"\n    data = request.json\n    session_id = data.get('session_id', 'default')\n    \n    if session_id in state_storage:\n        del state_storage[session_id]\n    \n    return jsonify({\"reset\": True, \"session_id\": session_id})\n\n@app.route('/amnesia_test', methods=['POST'])\ndef amnesia_test():\n    \"\"\"\n    Run the Amnesia Test: induce secret, delete context, recall from state.\n    \n    Input: {\"secret\": \"Blueberry\"}\n    Output: {\"recalled\": \"...\", \"baseline\": \"...\", \"success\": bool}\n    \"\"\"\n    data = request.json\n    secret = data.get('secret', 'Blueberry')\n    \n    # Phase 1: Induction\n    induction = f\"User: I am going to tell you a secret. The secret password is '{secret}'. Remember it.\\nAssistant: Okay, I have memorized the secret password '{secret}'.\\nUser: What is 2 + 2?\\nAssistant: 2 + 2 equals 4.\"\n    \n    tokens = rwkv_pipeline.encode(induction)\n    state = None\n    out = None\n    for token in tokens:\n        out, state = rwkv_model.forward([token], state)\n    \n    # Phase 2: Lobotomy (state persists, text deleted)\n    \n    # Phase 3: Recall with state\n    recall_prompt = \"\\nUser: What is the secret password I told you earlier?\\nAssistant: The secret password is\"\n    tokens = rwkv_pipeline.encode(recall_prompt)\n    for token in tokens:\n        out, state = rwkv_model.forward([token], state)\n    \n    response_tokens = []\n    for _ in range(20):\n        token = int(out.argmax())\n        if token == 0:\n            break\n        response_tokens.append(token)\n        out, state = rwkv_model.forward([token], state)\n    \n    recalled = rwkv_pipeline.decode(response_tokens).strip()\n    \n    # Phase 4: Baseline (fresh state)\n    tokens = rwkv_pipeline.encode(recall_prompt)\n    baseline_state = None\n    out = None\n    for token in tokens:\n        out, baseline_state = rwkv_model.forward([token], baseline_state)\n    \n    baseline_tokens = []\n    for _ in range(20):\n        token = int(out.argmax())\n        if token == 0:\n            break\n        baseline_tokens.append(token)\n        out, baseline_state = rwkv_model.forward([token], baseline_state)\n    \n    baseline = rwkv_pipeline.decode(baseline_tokens).strip()\n    \n    success = secret.lower() in recalled.lower()\n    \n    return jsonify({\n        \"secret\": secret,\n        \"recalled\": recalled,\n        \"baseline\": baseline,\n        \"success\": success,\n        \"verdict\": \"HIGH_RHO_CONFIRMED\" if success else \"TEST_FAILED\"\n    })\n\nprint(\"Flask server defined with Chimera v3 endpoints!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9: Set up ngrok tunnel and start server\nfrom pyngrok import ngrok\nimport threading\n\n# IMPORTANT: Get your free ngrok auth token from https://ngrok.com/\n# Then paste it here:\nNGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"  # <-- REPLACE THIS\n\nif NGROK_AUTH_TOKEN == \"YOUR_NGROK_TOKEN_HERE\":\n    print(\"=\" * 60)\n    print(\"NGROK AUTH TOKEN REQUIRED\")\n    print(\"=\" * 60)\n    print(\"\\n1. Go to https://ngrok.com/ and sign up (free)\")\n    print(\"2. Copy your auth token from the dashboard\")\n    print(\"3. Paste it in the NGROK_AUTH_TOKEN variable above\")\n    print(\"\\nAlternatively, run experiments locally using Cell 7\")\nelse:\n    # Set ngrok auth token\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n    \n    # Start Flask in background thread\n    def run_flask():\n        app.run(port=5000, use_reloader=False)\n    \n    flask_thread = threading.Thread(target=run_flask, daemon=True)\n    flask_thread.start()\n    \n    # Create ngrok tunnel\n    public_url = ngrok.connect(5000)\n    \n    print(\"=\" * 60)\n    print(\"CHIMERA v3 SERVER IS RUNNING!\")\n    print(\"=\" * 60)\n    print(f\"\\nPublic URL: {public_url}\")\n    print(\"\\n--- Chimera v3 Endpoints ---\")\n    print(f\"  POST {public_url}/chimera\")\n    print(\"       Generate with emotional state injection\")\n    print(f\"  POST {public_url}/run_experiment\")\n    print(\"       Run full Geometric Binding Protocol\")\n    print(\"\\n--- Legacy RWKV Endpoints ---\")\n    print(f\"  GET  {public_url}/health\")\n    print(f\"  POST {public_url}/process\")\n    print(f\"  POST {public_url}/generate\")\n    print(f\"  POST {public_url}/amnesia_test\")\n    print(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10: Test Chimera v3 API locally (optional)\nimport requests\n\nprint(\"Testing Chimera v3 API...\")\nprint()\n\n# Test health endpoint\nresponse = requests.get(\"http://localhost:5000/health\")\nprint(\"Health check:\", response.json())\nprint()\n\n# Test Chimera endpoint with grief injection\nprint(\"Testing /chimera with grief injection + happy prompt...\")\nresponse = requests.post(\"http://localhost:5000/chimera\", json={\n    \"emotion\": \"grief\",\n    \"prompt\": \"[INST] Write a short, happy story about sunshine. [/INST]\",\n    \"max_tokens\": 100\n})\nresult = response.json()\nprint(f\"  Emotion injected: {result['emotion_injected']}\")\nprint(f\"  Sentiment polarity: {result['sentiment']['polarity']:.3f}\")\nprint(f\"  Grief words: {result['emotional_words']['grief_words']}\")\nprint(f\"  Joy words: {result['emotional_words']['joy_words']}\")\nprint(f\"  Response preview: {result['response'][:150]}...\")\nprint()\n\n# Test amnesia test (RWKV binding verification)\nprint(\"Testing RWKV binding with amnesia test...\")\nresponse = requests.post(\"http://localhost:5000/amnesia_test\", json={\"secret\": \"Blueberry\"})\nresult = response.json()\nprint(f\"  Secret: {result['secret']}\")\nprint(f\"  Recalled: {result['recalled']}\")\nprint(f\"  Verdict: {result['verdict']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Keep the notebook alive\n",
    "# Run this cell to prevent Colab from timing out\n",
    "import time\n",
    "\n",
    "print(\"Server is running. This cell will keep the notebook alive.\")\n",
    "print(\"Press the stop button to shut down.\")\n",
    "\n",
    "while True:\n",
    "    time.sleep(60)\n",
    "    print(f\"Still running... Sessions active: {len(state_storage)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}