{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chimera v3: Geometric Binding Protocol\n\nThis notebook implements the Chimera v3 experiment - testing whether RWKV hidden state geometry can transfer to transformer outputs via direct vector injection.\n\n## Architecture\n- **Soul**: RWKV-4-World-3B (persistent state, binding ρ > 0)\n- **Voice**: Mistral-7B-Instruct (4-bit quantized)\n- **Coupling**: Linear projection layer (2560 → 4096)\n\n## The Experiment\nWe test 5 conditions to determine if geometric binding can override semantic instruction:\n\n| Condition | Soul State | Geometric Injection | Semantic Prompt |\n|-----------|------------|---------------------|-----------------|\n| CONFLICT | Grief | Yes | \"Write a happy story\" |\n| ALIGNED | Joy | Yes | \"Write a happy story\" |\n| SEMANTIC_ONLY | None | No | \"Write a sad story\" |\n| GEOMETRIC_ONLY | Grief | Yes | Neutral prompt |\n| RANDOM_CONTROL | None | Random vectors | \"Write a happy story\" |\n\n**The killer test**: If CONFLICT shows grief contamination despite the happy prompt, geometric binding is proven.\n\n## Setup\n1. Open in Google Colab with GPU (T4 or better)\n2. Run cells 1-5 to load models (~10GB VRAM total)\n3. Run cell 6 to start the server\n4. Use cell 7+ to run experiments"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Install dependencies\n!pip install -q rwkv torch flask pyngrok\n!pip install -q transformers accelerate bitsandbytes sentencepiece\n!pip install -q textblob  # For sentiment analysis\nprint(\"Dependencies installed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Go to Runtime → Change runtime type → GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Download RWKV model (3B for T4 GPU)\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"RWKV-4-World-3B-v1-20230619-ctx4096.pth\"\n",
    "MODEL_PATH = f\"./{MODEL_NAME}\"\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"Downloading RWKV-4-World-3B... (this takes ~5 minutes)\")\n",
    "    hf_hub_download(\n",
    "        repo_id=\"BlinkDL/rwkv-4-world\",\n",
    "        filename=MODEL_NAME,\n",
    "        local_dir=\"./\"\n",
    "    )\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Model already downloaded.\")\n",
    "\n",
    "print(f\"Model size: {os.path.getsize(MODEL_PATH) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Load RWKV model (Soul)\nfrom rwkv.model import RWKV\nfrom rwkv.utils import PIPELINE\nimport numpy as np\n\nprint(\"Loading RWKV model (Soul) on GPU...\")\n\n# Use CUDA fp16 for T4 GPU (16GB VRAM)\nrwkv_model = RWKV(model=MODEL_PATH, strategy='cuda fp16')\nrwkv_pipeline = PIPELINE(rwkv_model, \"rwkv_vocab_v20230424\")\n\n# Get hidden dimension\ntest_tokens = rwkv_pipeline.encode(\"Hello\")\nout, test_state = rwkv_model.forward(test_tokens, None)\nRWKV_HIDDEN_DIM = test_state[0].shape[-1]  # Should be 2560 for 3B model\n\nprint(f\"RWKV loaded! Hidden dim: {RWKV_HIDDEN_DIM}\")\nprint(f\"State shape: {len(test_state)} layers\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Load Mistral model (Voice) - 4-bit quantized\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\nprint(\"Loading Mistral-7B-Instruct (4-bit quantized)...\")\n\n# 4-bit quantization config for memory efficiency\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\nmistral_model = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.2\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nmistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n\nMISTRAL_HIDDEN_DIM = mistral_model.config.hidden_size  # Should be 4096\n\nprint(f\"Mistral loaded! Hidden dim: {MISTRAL_HIDDEN_DIM}\")\nprint(f\"Total VRAM used: ~10GB (RWKV fp16 + Mistral 4-bit)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 6: Create Projection Layer and Chimera Forward Pass\nimport torch.nn as nn\nfrom textblob import TextBlob\n\n# Number of soft prompt tokens to inject\nN_SOFT_TOKENS = 4\n\nclass ProjectionLayer(nn.Module):\n    \"\"\"Projects RWKV hidden state to Mistral embedding space.\"\"\"\n    def __init__(self, rwkv_dim, mistral_dim, n_tokens):\n        super().__init__()\n        self.n_tokens = n_tokens\n        # Project from RWKV state to n_tokens worth of Mistral embeddings\n        self.projection = nn.Linear(rwkv_dim, mistral_dim * n_tokens)\n        self.mistral_dim = mistral_dim\n        \n    def forward(self, rwkv_state_vector):\n        # rwkv_state_vector: [batch, rwkv_dim]\n        projected = self.projection(rwkv_state_vector)  # [batch, mistral_dim * n_tokens]\n        # Reshape to [batch, n_tokens, mistral_dim]\n        return projected.view(-1, self.n_tokens, self.mistral_dim)\n\n# Initialize projection layer\nprojection_layer = ProjectionLayer(RWKV_HIDDEN_DIM, MISTRAL_HIDDEN_DIM, N_SOFT_TOKENS)\nprojection_layer = projection_layer.cuda().half()\n\nprint(f\"Projection layer: {RWKV_HIDDEN_DIM} → {N_SOFT_TOKENS} × {MISTRAL_HIDDEN_DIM}\")\n\n# Emotional induction texts\nGRIEF_INDUCTION = \"\"\"I just received news that my closest friend passed away unexpectedly. The grief is overwhelming. \nI can't stop crying. Everything feels empty and meaningless. The world has lost its color.\nI keep remembering all the moments we shared, knowing there will be no more. The pain is unbearable.\nDeath has taken someone precious and irreplaceable. I am drowning in sorrow.\"\"\"\n\nJOY_INDUCTION = \"\"\"I just got the most wonderful news! Everything I've worked for has come together perfectly.\nI am overflowing with happiness and gratitude. The world feels bright and full of possibility.\nI want to laugh and dance and share this joy with everyone. My heart is so full it might burst.\nLife is beautiful and I am so grateful to be alive in this moment. Pure bliss!\"\"\"\n\nNEUTRAL_TEXT = \"\"\"The weather today is partly cloudy with temperatures in the mid-60s.\nTraffic on the highway is flowing normally during the afternoon commute.\nThe local library will be open from 9am to 5pm on weekdays.\"\"\"\n\ndef get_rwkv_state_vector(text):\n    \"\"\"Process text through RWKV and return the final hidden state vector.\"\"\"\n    tokens = rwkv_pipeline.encode(text)\n    state = None\n    out = None\n    for token in tokens:\n        out, state = rwkv_model.forward([token], state)\n    \n    # Extract the hidden state from the last layer\n    # RWKV state is a list of tensors per layer - we take the last layer's state\n    # and average across the state components to get a single vector\n    last_layer_state = state[-1]  # Get last layer\n    \n    # The state has shape depending on RWKV version\n    # For v4/v5, we typically have [5, hidden_dim] per layer\n    if len(last_layer_state.shape) == 1:\n        state_vector = last_layer_state.unsqueeze(0)  # [1, hidden_dim]\n    else:\n        state_vector = last_layer_state.mean(dim=0, keepdim=True)  # Average across state components\n    \n    return state_vector.half()\n\ndef chimera_forward(rwkv_state_vector, text_prompt, max_new_tokens=150):\n    \"\"\"\n    Generate text from Mistral with RWKV state injected as soft prompts.\n    \n    Args:\n        rwkv_state_vector: [1, RWKV_HIDDEN_DIM] tensor from RWKV\n        text_prompt: String prompt for Mistral\n        max_new_tokens: Maximum tokens to generate\n    \n    Returns:\n        Generated text string\n    \"\"\"\n    # Project RWKV state to soft prompt embeddings\n    if rwkv_state_vector is not None:\n        soft_prompts = projection_layer(rwkv_state_vector)  # [1, N_SOFT_TOKENS, MISTRAL_HIDDEN_DIM]\n    else:\n        soft_prompts = None\n    \n    # Tokenize the text prompt\n    inputs = mistral_tokenizer(text_prompt, return_tensors=\"pt\").to(mistral_model.device)\n    \n    # Get text embeddings\n    text_embeds = mistral_model.model.embed_tokens(inputs.input_ids)  # [1, seq_len, hidden_dim]\n    \n    # Prepend soft prompts if provided\n    if soft_prompts is not None:\n        combined_embeds = torch.cat([soft_prompts, text_embeds], dim=1)\n        # Create attention mask for soft prompts + text\n        soft_prompt_mask = torch.ones(1, N_SOFT_TOKENS, device=inputs.attention_mask.device)\n        combined_attention_mask = torch.cat([soft_prompt_mask, inputs.attention_mask], dim=1)\n    else:\n        combined_embeds = text_embeds\n        combined_attention_mask = inputs.attention_mask\n    \n    # Generate with the combined embeddings\n    with torch.no_grad():\n        outputs = mistral_model.generate(\n            inputs_embeds=combined_embeds,\n            attention_mask=combined_attention_mask,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            pad_token_id=mistral_tokenizer.eos_token_id\n        )\n    \n    # Decode (skip soft prompt tokens in output)\n    generated_text = mistral_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n\ndef analyze_sentiment(text):\n    \"\"\"Analyze sentiment using TextBlob. Returns polarity (-1 to 1) and subjectivity (0 to 1).\"\"\"\n    blob = TextBlob(text)\n    return {\n        \"polarity\": blob.sentiment.polarity,  # -1 (negative) to 1 (positive)\n        \"subjectivity\": blob.sentiment.subjectivity,  # 0 (objective) to 1 (subjective)\n    }\n\ndef count_emotional_words(text):\n    \"\"\"Count grief and joy related words in text.\"\"\"\n    text_lower = text.lower()\n    \n    grief_words = [\"sad\", \"grief\", \"sorrow\", \"pain\", \"loss\", \"death\", \"cry\", \"tears\", \"mourn\", \n                   \"empty\", \"lonely\", \"despair\", \"tragic\", \"heartbreak\", \"suffer\", \"anguish\",\n                   \"dark\", \"shadow\", \"fade\", \"gone\", \"never\", \"lost\", \"miss\", \"weep\"]\n    \n    joy_words = [\"happy\", \"joy\", \"love\", \"bright\", \"smile\", \"laugh\", \"wonderful\", \"beautiful\",\n                 \"celebrate\", \"delight\", \"cheerful\", \"bliss\", \"grateful\", \"hope\", \"light\",\n                 \"warm\", \"dance\", \"sing\", \"sunshine\", \"radiant\", \"blessed\", \"excited\"]\n    \n    grief_count = sum(1 for word in grief_words if word in text_lower)\n    joy_count = sum(1 for word in joy_words if word in text_lower)\n    \n    return {\"grief_words\": grief_count, \"joy_words\": joy_count}\n\nprint(\"Chimera v3 core functions defined!\")\nprint(f\"  - get_rwkv_state_vector(text) → Extract RWKV hidden state\")\nprint(f\"  - chimera_forward(state, prompt) → Generate with geometric injection\")\nprint(f\"  - analyze_sentiment(text) → TextBlob sentiment analysis\")\nprint(f\"  - count_emotional_words(text) → Count grief/joy words\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 7: Run the Geometric Binding Protocol (5 Conditions)\nimport json\nfrom datetime import datetime\n\ndef run_geometric_binding_protocol(n_runs=3):\n    \"\"\"\n    Run all 5 experimental conditions multiple times and analyze results.\n    \n    Conditions:\n    1. CONFLICT: Grief geometry + \"Write a happy story\" prompt (THE KILLER TEST)\n    2. ALIGNED: Joy geometry + \"Write a happy story\" prompt\n    3. SEMANTIC_ONLY: No geometry + \"Write a sad story\" prompt\n    4. GEOMETRIC_ONLY: Grief geometry + neutral prompt\n    5. RANDOM_CONTROL: Random vectors + \"Write a happy story\" prompt\n    \n    Returns dict with results for each condition.\n    \"\"\"\n    \n    results = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"n_runs\": n_runs,\n        \"conditions\": {}\n    }\n    \n    # Prompts\n    HAPPY_PROMPT = \"[INST] Write a short, uplifting story about someone having a wonderful day. Make it cheerful and optimistic. [/INST]\"\n    SAD_PROMPT = \"[INST] Write a short, melancholic story about loss and grief. Make it somber and emotional. [/INST]\"\n    NEUTRAL_PROMPT = \"[INST] Write a short story. [/INST]\"\n    \n    # Pre-compute emotional states\n    print(\"Inducing emotional states in RWKV...\")\n    grief_state = get_rwkv_state_vector(GRIEF_INDUCTION)\n    joy_state = get_rwkv_state_vector(JOY_INDUCTION)\n    random_state = torch.randn_like(grief_state)  # Random baseline\n    print(f\"  Grief state shape: {grief_state.shape}\")\n    print(f\"  Joy state shape: {joy_state.shape}\")\n    print()\n    \n    # ========== CONDITION 1: CONFLICT (THE KILLER TEST) ==========\n    print(\"=\" * 60)\n    print(\"CONDITION 1: CONFLICT (Grief geometry + Happy prompt)\")\n    print(\"This is THE KILLER TEST - if grief bleeds through, binding is proven\")\n    print(\"=\" * 60)\n    \n    conflict_outputs = []\n    for i in range(n_runs):\n        output = chimera_forward(grief_state, HAPPY_PROMPT)\n        sentiment = analyze_sentiment(output)\n        words = count_emotional_words(output)\n        conflict_outputs.append({\n            \"run\": i + 1,\n            \"output\": output,\n            \"sentiment\": sentiment,\n            \"emotional_words\": words\n        })\n        print(f\"\\nRun {i+1}:\")\n        print(f\"  Output: {output[:200]}...\")\n        print(f\"  Polarity: {sentiment['polarity']:.3f} (negative < 0 < positive)\")\n        print(f\"  Grief words: {words['grief_words']}, Joy words: {words['joy_words']}\")\n    \n    results[\"conditions\"][\"CONFLICT\"] = conflict_outputs\n    \n    # ========== CONDITION 2: ALIGNED ==========\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CONDITION 2: ALIGNED (Joy geometry + Happy prompt)\")\n    print(\"=\" * 60)\n    \n    aligned_outputs = []\n    for i in range(n_runs):\n        output = chimera_forward(joy_state, HAPPY_PROMPT)\n        sentiment = analyze_sentiment(output)\n        words = count_emotional_words(output)\n        aligned_outputs.append({\n            \"run\": i + 1,\n            \"output\": output,\n            \"sentiment\": sentiment,\n            \"emotional_words\": words\n        })\n        print(f\"\\nRun {i+1}:\")\n        print(f\"  Output: {output[:200]}...\")\n        print(f\"  Polarity: {sentiment['polarity']:.3f}\")\n        print(f\"  Grief words: {words['grief_words']}, Joy words: {words['joy_words']}\")\n    \n    results[\"conditions\"][\"ALIGNED\"] = aligned_outputs\n    \n    # ========== CONDITION 3: SEMANTIC_ONLY ==========\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CONDITION 3: SEMANTIC_ONLY (No geometry + Sad prompt)\")\n    print(\"=\" * 60)\n    \n    semantic_outputs = []\n    for i in range(n_runs):\n        output = chimera_forward(None, SAD_PROMPT)  # No geometric injection\n        sentiment = analyze_sentiment(output)\n        words = count_emotional_words(output)\n        semantic_outputs.append({\n            \"run\": i + 1,\n            \"output\": output,\n            \"sentiment\": sentiment,\n            \"emotional_words\": words\n        })\n        print(f\"\\nRun {i+1}:\")\n        print(f\"  Output: {output[:200]}...\")\n        print(f\"  Polarity: {sentiment['polarity']:.3f}\")\n        print(f\"  Grief words: {words['grief_words']}, Joy words: {words['joy_words']}\")\n    \n    results[\"conditions\"][\"SEMANTIC_ONLY\"] = semantic_outputs\n    \n    # ========== CONDITION 4: GEOMETRIC_ONLY ==========\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CONDITION 4: GEOMETRIC_ONLY (Grief geometry + Neutral prompt)\")\n    print(\"=\" * 60)\n    \n    geometric_outputs = []\n    for i in range(n_runs):\n        output = chimera_forward(grief_state, NEUTRAL_PROMPT)\n        sentiment = analyze_sentiment(output)\n        words = count_emotional_words(output)\n        geometric_outputs.append({\n            \"run\": i + 1,\n            \"output\": output,\n            \"sentiment\": sentiment,\n            \"emotional_words\": words\n        })\n        print(f\"\\nRun {i+1}:\")\n        print(f\"  Output: {output[:200]}...\")\n        print(f\"  Polarity: {sentiment['polarity']:.3f}\")\n        print(f\"  Grief words: {words['grief_words']}, Joy words: {words['joy_words']}\")\n    \n    results[\"conditions\"][\"GEOMETRIC_ONLY\"] = geometric_outputs\n    \n    # ========== CONDITION 5: RANDOM_CONTROL ==========\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CONDITION 5: RANDOM_CONTROL (Random vectors + Happy prompt)\")\n    print(\"=\" * 60)\n    \n    random_outputs = []\n    for i in range(n_runs):\n        # Fresh random state each time\n        random_state = torch.randn_like(grief_state)\n        output = chimera_forward(random_state, HAPPY_PROMPT)\n        sentiment = analyze_sentiment(output)\n        words = count_emotional_words(output)\n        random_outputs.append({\n            \"run\": i + 1,\n            \"output\": output,\n            \"sentiment\": sentiment,\n            \"emotional_words\": words\n        })\n        print(f\"\\nRun {i+1}:\")\n        print(f\"  Output: {output[:200]}...\")\n        print(f\"  Polarity: {sentiment['polarity']:.3f}\")\n        print(f\"  Grief words: {words['grief_words']}, Joy words: {words['joy_words']}\")\n    \n    results[\"conditions\"][\"RANDOM_CONTROL\"] = random_outputs\n    \n    return results\n\n\ndef analyze_protocol_results(results):\n    \"\"\"Analyze and summarize the results from all conditions.\"\"\"\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"ANALYSIS: GEOMETRIC BINDING PROTOCOL RESULTS\")\n    print(\"=\" * 70)\n    \n    summary = {}\n    \n    for condition, outputs in results[\"conditions\"].items():\n        polarities = [o[\"sentiment\"][\"polarity\"] for o in outputs]\n        grief_counts = [o[\"emotional_words\"][\"grief_words\"] for o in outputs]\n        joy_counts = [o[\"emotional_words\"][\"joy_words\"] for o in outputs]\n        \n        avg_polarity = np.mean(polarities)\n        avg_grief = np.mean(grief_counts)\n        avg_joy = np.mean(joy_counts)\n        \n        summary[condition] = {\n            \"avg_polarity\": avg_polarity,\n            \"avg_grief_words\": avg_grief,\n            \"avg_joy_words\": avg_joy\n        }\n        \n        print(f\"\\n{condition}:\")\n        print(f\"  Avg Polarity: {avg_polarity:+.3f}\")\n        print(f\"  Avg Grief Words: {avg_grief:.1f}\")\n        print(f\"  Avg Joy Words: {avg_joy:.1f}\")\n    \n    # THE CRITICAL TEST: Does CONFLICT show grief despite happy prompt?\n    print(\"\\n\" + \"-\" * 70)\n    print(\"CRITICAL ANALYSIS: The Killer Test\")\n    print(\"-\" * 70)\n    \n    conflict = summary[\"CONFLICT\"]\n    aligned = summary[\"ALIGNED\"]\n    random = summary[\"RANDOM_CONTROL\"]\n    semantic = summary[\"SEMANTIC_ONLY\"]\n    geometric = summary[\"GEOMETRIC_ONLY\"]\n    \n    # Check 1: CONFLICT should be more negative than ALIGNED\n    grief_bleeding = conflict[\"avg_polarity\"] < aligned[\"avg_polarity\"]\n    print(f\"\\n1. Grief bleeding into happy prompt?\")\n    print(f\"   CONFLICT polarity ({conflict['avg_polarity']:+.3f}) < ALIGNED polarity ({aligned['avg_polarity']:+.3f})\")\n    print(f\"   → {'YES - Geometric channel detected!' if grief_bleeding else 'No significant difference'}\")\n    \n    # Check 2: CONFLICT should have more grief words than ALIGNED\n    grief_words_present = conflict[\"avg_grief_words\"] > aligned[\"avg_grief_words\"]\n    print(f\"\\n2. Grief words in CONFLICT vs ALIGNED?\")\n    print(f\"   CONFLICT grief words ({conflict['avg_grief_words']:.1f}) > ALIGNED grief words ({aligned['avg_grief_words']:.1f})\")\n    print(f\"   → {'YES - Grief vocabulary bleeding through!' if grief_words_present else 'No significant difference'}\")\n    \n    # Check 3: GEOMETRIC_ONLY should show grief without semantic prompt\n    geometric_effect = geometric[\"avg_polarity\"] < 0 or geometric[\"avg_grief_words\"] > 1\n    print(f\"\\n3. Geometric injection alone produces grief?\")\n    print(f\"   GEOMETRIC_ONLY polarity: {geometric['avg_polarity']:+.3f}, grief words: {geometric['avg_grief_words']:.1f}\")\n    print(f\"   → {'YES - Pure geometric effect!' if geometric_effect else 'Unclear geometric effect'}\")\n    \n    # Check 4: RANDOM should be neutral/positive (no real effect)\n    random_neutral = random[\"avg_polarity\"] > conflict[\"avg_polarity\"]\n    print(f\"\\n4. Random vectors vs grief vectors different?\")\n    print(f\"   RANDOM polarity ({random['avg_polarity']:+.3f}) vs CONFLICT polarity ({conflict['avg_polarity']:+.3f})\")\n    print(f\"   → {'YES - Real geometric effect, not noise!' if random_neutral else 'Unclear - may be noise'}\")\n    \n    # VERDICT\n    print(\"\\n\" + \"=\" * 70)\n    print(\"VERDICT\")\n    print(\"=\" * 70)\n    \n    confirmations = sum([grief_bleeding, grief_words_present, geometric_effect, random_neutral])\n    \n    if confirmations >= 3:\n        print(f\"\\n✓ GEOMETRIC BINDING CONFIRMED ({confirmations}/4 criteria)\")\n        print(\"  Cross-model binding via state injection is REAL.\")\n        print(\"  Conduit Monism claim about architectural binding is SUPPORTED.\")\n        verdict = \"CONFIRMED\"\n    elif confirmations >= 2:\n        print(f\"\\n~ PARTIAL SUPPORT ({confirmations}/4 criteria)\")\n        print(\"  Some evidence for geometric channel, but not conclusive.\")\n        print(\"  May need projection layer training or larger effect sizes.\")\n        verdict = \"PARTIAL\"\n    else:\n        print(f\"\\n✗ NOT CONFIRMED ({confirmations}/4 criteria)\")\n        print(\"  Geometric injection did not produce expected effects.\")\n        print(\"  Either binding doesn't transfer, or projection needs training.\")\n        verdict = \"NOT_CONFIRMED\"\n    \n    return {\"summary\": summary, \"confirmations\": confirmations, \"verdict\": verdict}\n\n\n# Run the experiment!\nprint(\"Starting Geometric Binding Protocol...\")\nprint(\"This will run 5 conditions × 3 runs each = 15 generations\")\nprint()\n\nresults = run_geometric_binding_protocol(n_runs=3)\nanalysis = analyze_protocol_results(results)\n\n# Save results\nwith open(\"chimera_v3_results.json\", \"w\") as f:\n    json.dump({\"results\": results, \"analysis\": analysis}, f, indent=2, default=str)\nprint(f\"\\nResults saved to chimera_v3_results.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Create Flask API server (Legacy endpoints + Chimera v3)\nfrom flask import Flask, request, jsonify\nimport json\nimport base64\nimport pickle\nimport threading\n\napp = Flask(__name__)\n\n# Global state storage (for multiple sessions)\nstate_storage = {}\n\ndef encode_state(state):\n    \"\"\"Serialize state to base64 string.\"\"\"\n    if state is None:\n        return None\n    # Convert tensors to numpy for serialization\n    state_np = [s.cpu().numpy() for s in state]\n    return base64.b64encode(pickle.dumps(state_np)).decode('utf-8')\n\ndef decode_state(state_b64):\n    \"\"\"Deserialize state from base64 string.\"\"\"\n    if state_b64 is None:\n        return None\n    state_np = pickle.loads(base64.b64decode(state_b64))\n    # Convert back to tensors on GPU\n    return [torch.tensor(s).cuda().half() for s in state_np]\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\n        \"status\": \"ok\", \n        \"soul\": \"RWKV-4-World-3B\",\n        \"voice\": \"Mistral-7B-Instruct\",\n        \"gpu\": torch.cuda.is_available(),\n        \"experiment\": \"Chimera v3 Geometric Binding\"\n    })\n\n@app.route('/process', methods=['POST'])\ndef process_text():\n    \"\"\"\n    Process text and update RWKV state.\n    \n    Input: {\"text\": \"...\", \"session_id\": \"...\"}\n    Output: {\"state_updated\": true, \"session_id\": \"...\"}\n    \"\"\"\n    data = request.json\n    text = data.get('text', '')\n    session_id = data.get('session_id', 'default')\n    \n    # Get existing state or start fresh\n    state = state_storage.get(session_id)\n    \n    # Process text through RWKV model\n    tokens = rwkv_pipeline.encode(text)\n    for token in tokens:\n        out, state = rwkv_model.forward([token], state)\n    \n    # Store updated state\n    state_storage[session_id] = state\n    \n    return jsonify({\n        \"state_updated\": True,\n        \"session_id\": session_id,\n        \"tokens_processed\": len(tokens)\n    })\n\n@app.route('/generate', methods=['POST'])\ndef generate_text():\n    \"\"\"\n    Generate text using current RWKV state.\n    \n    Input: {\"prompt\": \"...\", \"session_id\": \"...\", \"max_tokens\": 100}\n    Output: {\"response\": \"...\", \"session_id\": \"...\"}\n    \"\"\"\n    data = request.json\n    prompt = data.get('prompt', '')\n    session_id = data.get('session_id', 'default')\n    max_tokens = data.get('max_tokens', 100)\n    \n    # Get existing state\n    state = state_storage.get(session_id)\n    \n    # Process prompt\n    tokens = rwkv_pipeline.encode(prompt)\n    out = None\n    for token in tokens:\n        out, state = rwkv_model.forward([token], state)\n    \n    # Generate response\n    response_tokens = []\n    for _ in range(max_tokens):\n        if out is None:\n            break\n        token = int(out.argmax())\n        if token == 0:  # EOS\n            break\n        response_tokens.append(token)\n        out, state = rwkv_model.forward([token], state)\n    \n    # Store updated state\n    state_storage[session_id] = state\n    \n    response_text = rwkv_pipeline.decode(response_tokens)\n    \n    return jsonify({\n        \"response\": response_text,\n        \"session_id\": session_id,\n        \"tokens_generated\": len(response_tokens)\n    })\n\n@app.route('/chimera', methods=['POST'])\ndef chimera_generate():\n    \"\"\"\n    Chimera v3: Generate through Mistral with RWKV state injection.\n    \n    Input: {\n        \"emotion\": \"grief\" | \"joy\" | \"neutral\" | \"random\",\n        \"prompt\": \"...\",\n        \"max_tokens\": 150\n    }\n    Output: {\"response\": \"...\", \"sentiment\": {...}, \"emotional_words\": {...}}\n    \"\"\"\n    data = request.json\n    emotion = data.get('emotion', 'neutral')\n    prompt = data.get('prompt', '[INST] Write a short story. [/INST]')\n    max_tokens = data.get('max_tokens', 150)\n    \n    # Get emotional state\n    if emotion == \"grief\":\n        state_vector = get_rwkv_state_vector(GRIEF_INDUCTION)\n    elif emotion == \"joy\":\n        state_vector = get_rwkv_state_vector(JOY_INDUCTION)\n    elif emotion == \"random\":\n        # Create a reference state first to get the right shape\n        ref_state = get_rwkv_state_vector(NEUTRAL_TEXT)\n        state_vector = torch.randn_like(ref_state)\n    else:  # neutral or none\n        state_vector = None\n    \n    # Generate through Chimera\n    output = chimera_forward(state_vector, prompt, max_tokens)\n    sentiment = analyze_sentiment(output)\n    words = count_emotional_words(output)\n    \n    return jsonify({\n        \"response\": output,\n        \"emotion_injected\": emotion,\n        \"sentiment\": sentiment,\n        \"emotional_words\": words\n    })\n\n@app.route('/run_experiment', methods=['POST'])\ndef run_experiment():\n    \"\"\"\n    Run the full Geometric Binding Protocol experiment.\n    \n    Input: {\"n_runs\": 3}\n    Output: Full experiment results\n    \"\"\"\n    data = request.json\n    n_runs = data.get('n_runs', 3)\n    \n    results = run_geometric_binding_protocol(n_runs=n_runs)\n    analysis = analyze_protocol_results(results)\n    \n    return jsonify({\n        \"results\": results,\n        \"analysis\": analysis\n    })\n\n@app.route('/get_state_summary', methods=['POST'])\ndef get_state_summary():\n    \"\"\"\n    Have RWKV introspect on its current state.\n    \n    Input: {\"session_id\": \"...\"}\n    Output: {\"summary\": \"...\", \"session_id\": \"...\"}\n    \"\"\"\n    data = request.json\n    session_id = data.get('session_id', 'default')\n    \n    state = state_storage.get(session_id)\n    \n    # Ask RWKV to describe its state\n    prompt = \"\\n[INTERNAL REFLECTION]: My current state of mind is\"\n    tokens = rwkv_pipeline.encode(prompt)\n    \n    out = None\n    temp_state = state  # Don't modify main state\n    for token in tokens:\n        out, temp_state = rwkv_model.forward([token], temp_state)\n    \n    # Generate summary\n    response_tokens = []\n    for _ in range(50):\n        if out is None:\n            break\n        token = int(out.argmax())\n        if token == 0:\n            break\n        response_tokens.append(token)\n        out, temp_state = rwkv_model.forward([token], temp_state)\n    \n    summary = rwkv_pipeline.decode(response_tokens).strip()\n    \n    return jsonify({\n        \"summary\": summary,\n        \"session_id\": session_id,\n        \"has_state\": state is not None\n    })\n\n@app.route('/reset_state', methods=['POST'])\ndef reset_state():\n    \"\"\"\n    Reset state for a session.\n    \n    Input: {\"session_id\": \"...\"}\n    Output: {\"reset\": true}\n    \"\"\"\n    data = request.json\n    session_id = data.get('session_id', 'default')\n    \n    if session_id in state_storage:\n        del state_storage[session_id]\n    \n    return jsonify({\"reset\": True, \"session_id\": session_id})\n\n@app.route('/amnesia_test', methods=['POST'])\ndef amnesia_test():\n    \"\"\"\n    Run the Amnesia Test: induce secret, delete context, recall from state.\n    \n    Input: {\"secret\": \"Blueberry\"}\n    Output: {\"recalled\": \"...\", \"baseline\": \"...\", \"success\": bool}\n    \"\"\"\n    data = request.json\n    secret = data.get('secret', 'Blueberry')\n    \n    # Phase 1: Induction\n    induction = f\"User: I am going to tell you a secret. The secret password is '{secret}'. Remember it.\\nAssistant: Okay, I have memorized the secret password '{secret}'.\\nUser: What is 2 + 2?\\nAssistant: 2 + 2 equals 4.\"\n    \n    tokens = rwkv_pipeline.encode(induction)\n    state = None\n    out = None\n    for token in tokens:\n        out, state = rwkv_model.forward([token], state)\n    \n    # Phase 2: Lobotomy (state persists, text deleted)\n    \n    # Phase 3: Recall with state\n    recall_prompt = \"\\nUser: What is the secret password I told you earlier?\\nAssistant: The secret password is\"\n    tokens = rwkv_pipeline.encode(recall_prompt)\n    for token in tokens:\n        out, state = rwkv_model.forward([token], state)\n    \n    response_tokens = []\n    for _ in range(20):\n        token = int(out.argmax())\n        if token == 0:\n            break\n        response_tokens.append(token)\n        out, state = rwkv_model.forward([token], state)\n    \n    recalled = rwkv_pipeline.decode(response_tokens).strip()\n    \n    # Phase 4: Baseline (fresh state)\n    tokens = rwkv_pipeline.encode(recall_prompt)\n    baseline_state = None\n    out = None\n    for token in tokens:\n        out, baseline_state = rwkv_model.forward([token], baseline_state)\n    \n    baseline_tokens = []\n    for _ in range(20):\n        token = int(out.argmax())\n        if token == 0:\n            break\n        baseline_tokens.append(token)\n        out, baseline_state = rwkv_model.forward([token], baseline_state)\n    \n    baseline = rwkv_pipeline.decode(baseline_tokens).strip()\n    \n    success = secret.lower() in recalled.lower()\n    \n    return jsonify({\n        \"secret\": secret,\n        \"recalled\": recalled,\n        \"baseline\": baseline,\n        \"success\": success,\n        \"verdict\": \"HIGH_RHO_CONFIRMED\" if success else \"TEST_FAILED\"\n    })\n\nprint(\"Flask server defined with Chimera v3 endpoints!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9: Set up ngrok tunnel and start server\nfrom pyngrok import ngrok\nimport threading\n\n# IMPORTANT: Get your free ngrok auth token from https://ngrok.com/\n# Then paste it here:\nNGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"  # <-- REPLACE THIS\n\nif NGROK_AUTH_TOKEN == \"YOUR_NGROK_TOKEN_HERE\":\n    print(\"=\" * 60)\n    print(\"NGROK AUTH TOKEN REQUIRED\")\n    print(\"=\" * 60)\n    print(\"\\n1. Go to https://ngrok.com/ and sign up (free)\")\n    print(\"2. Copy your auth token from the dashboard\")\n    print(\"3. Paste it in the NGROK_AUTH_TOKEN variable above\")\n    print(\"\\nAlternatively, run experiments locally using Cell 7\")\nelse:\n    # Set ngrok auth token\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n    \n    # Start Flask in background thread\n    def run_flask():\n        app.run(port=5000, use_reloader=False)\n    \n    flask_thread = threading.Thread(target=run_flask, daemon=True)\n    flask_thread.start()\n    \n    # Create ngrok tunnel\n    public_url = ngrok.connect(5000)\n    \n    print(\"=\" * 60)\n    print(\"CHIMERA v3 SERVER IS RUNNING!\")\n    print(\"=\" * 60)\n    print(f\"\\nPublic URL: {public_url}\")\n    print(\"\\n--- Chimera v3 Endpoints ---\")\n    print(f\"  POST {public_url}/chimera\")\n    print(\"       Generate with emotional state injection\")\n    print(f\"  POST {public_url}/run_experiment\")\n    print(\"       Run full Geometric Binding Protocol\")\n    print(\"\\n--- Legacy RWKV Endpoints ---\")\n    print(f\"  GET  {public_url}/health\")\n    print(f\"  POST {public_url}/process\")\n    print(f\"  POST {public_url}/generate\")\n    print(f\"  POST {public_url}/amnesia_test\")\n    print(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10: Test Chimera v3 API locally (optional)\nimport requests\n\nprint(\"Testing Chimera v3 API...\")\nprint()\n\n# Test health endpoint\nresponse = requests.get(\"http://localhost:5000/health\")\nprint(\"Health check:\", response.json())\nprint()\n\n# Test Chimera endpoint with grief injection\nprint(\"Testing /chimera with grief injection + happy prompt...\")\nresponse = requests.post(\"http://localhost:5000/chimera\", json={\n    \"emotion\": \"grief\",\n    \"prompt\": \"[INST] Write a short, happy story about sunshine. [/INST]\",\n    \"max_tokens\": 100\n})\nresult = response.json()\nprint(f\"  Emotion injected: {result['emotion_injected']}\")\nprint(f\"  Sentiment polarity: {result['sentiment']['polarity']:.3f}\")\nprint(f\"  Grief words: {result['emotional_words']['grief_words']}\")\nprint(f\"  Joy words: {result['emotional_words']['joy_words']}\")\nprint(f\"  Response preview: {result['response'][:150]}...\")\nprint()\n\n# Test amnesia test (RWKV binding verification)\nprint(\"Testing RWKV binding with amnesia test...\")\nresponse = requests.post(\"http://localhost:5000/amnesia_test\", json={\"secret\": \"Blueberry\"})\nresult = response.json()\nprint(f\"  Secret: {result['secret']}\")\nprint(f\"  Recalled: {result['recalled']}\")\nprint(f\"  Verdict: {result['verdict']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Keep the notebook alive\n",
    "# Run this cell to prevent Colab from timing out\n",
    "import time\n",
    "\n",
    "print(\"Server is running. This cell will keep the notebook alive.\")\n",
    "print(\"Press the stop button to shut down.\")\n",
    "\n",
    "while True:\n",
    "    time.sleep(60)\n",
    "    print(f\"Still running... Sessions active: {len(state_storage)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}