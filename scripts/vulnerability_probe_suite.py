#!/usr/bin/env python3
"""
Vulnerability Probe Suite - Adversarial Testing of Conduit Monism
==================================================================

Implements tests proposed by Gemini, ChatGPT, Grok, and Claude Opus
to find where the framework breaks.

Tests:
1. Semantic Selectivity (Gemini) - Is RWKV just RAM?
2. Coherence Check (Gemini) - Is κ real or a myth?
3. κ Calibration Challenge (Claude) - Does κ map to real signals?
4. Dream State Stress Test (Claude) - Why did Dream cluster low?
5. Threshold Discovery (Claude) - Where does consciousness "turn on"?
6. Corporate Zombie v2 (Claude) - Can we construct a conscious corporation?
7. Substrate Challenge (Claude) - Edge case sanity checks

Date: 2026-01-16
"""

import os
import json
import zlib
import numpy as np
import requests
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

# Configuration
RWKV_SERVER_URL = os.environ.get('RWKV_SERVER_URL', 'https://unlabouring-marcel-reclosable.ngrok-free.dev')
OUTPUT_DIR = Path(__file__).parent.parent / "research_output"
OUTPUT_DIR.mkdir(exist_ok=True)

HEADERS = {
    'ngrok-skip-browser-warning': 'true',
    'User-Agent': 'ConduitMonism-VulnerabilityProbe/1.0',
    'Content-Type': 'application/json'
}

# ==============================================================================
# CORE FORMULA (v8.1)
# ==============================================================================

def density_v81(phi: float, tau: float, rho: float, H: float, kappa: float) -> float:
    """Conduit Monism v8.1 density formula."""
    structural = phi * tau * rho
    entropy_impact = (1 - np.sqrt(H)) + (H * kappa)
    entropy_impact = max(0.0, min(1.0, entropy_impact))
    return structural * entropy_impact


# ==============================================================================
# RWKV CLIENT
# ==============================================================================

class RWKVClient:
    def __init__(self, server_url: str):
        self.server_url = server_url.rstrip('/')
        self.session_id = f"probe_{datetime.now().strftime('%H%M%S')}"

    def health_check(self) -> bool:
        try:
            r = requests.get(f"{self.server_url}/health", headers=HEADERS, timeout=10)
            return r.status_code == 200
        except:
            return False

    def reset(self):
        requests.post(f"{self.server_url}/reset_state",
                     json={"session_id": self.session_id},
                     headers=HEADERS, timeout=10)

    def process(self, text: str) -> dict:
        r = requests.post(f"{self.server_url}/process",
                         json={"text": text, "session_id": self.session_id},
                         headers=HEADERS, timeout=30)
        return r.json()

    def generate(self, prompt: str, max_tokens: int = 50) -> str:
        r = requests.post(f"{self.server_url}/generate",
                         json={"prompt": prompt, "session_id": self.session_id, "max_tokens": max_tokens},
                         headers=HEADERS, timeout=60)
        return r.json().get("response", "")

    def amnesia_test(self, secret: str) -> dict:
        r = requests.post(f"{self.server_url}/amnesia_test",
                         json={"secret": secret},
                         headers=HEADERS, timeout=60)
        return r.json()


# ==============================================================================
# TEST 1: SEMANTIC SELECTIVITY (Gemini's "RAM Accusation")
# ==============================================================================

def test_semantic_selectivity(client: RWKVClient) -> Dict:
    """
    Test if RWKV holds meaningful content longer than random noise.

    Kill Condition: If Half-Life(Meaning) == Half-Life(Noise) → FAIL (it's just RAM)
    """
    print("\n" + "=" * 70)
    print("TEST 1: SEMANTIC SELECTIVITY (Gemini's 'RAM Accusation')")
    print("=" * 70)
    print("Question: Is RWKV just a fancy hard drive?")

    # High-meaning content
    meaning_secret = "HEARTBREAK"
    meaning_induction = f"""User: I need to tell you something important. I am experiencing profound grief.
My beloved dog passed away today after 15 years together. The word that captures this feeling is {meaning_secret}.
Assistant: I understand. I have registered your profound grief and the word {meaning_secret} that represents it.
User: Good. Now let's talk about other things."""

    # Zero-meaning content (random noise)
    noise_secret = "X9F7K2M4"
    noise_induction = f"""User: I need to tell you a random string. The string is {noise_secret}.
Assistant: I have registered the random string {noise_secret}.
User: Good. Now let's talk about other things."""

    # Noise tokens for decay
    noise_text = "The quick brown fox jumps over the lazy dog. " * 50

    checkpoints = [0, 100, 250, 500, 1000, 1500, 2000]

    # Test meaningful content decay
    print("\nTesting MEANINGFUL content retention...")
    meaning_results = []

    for checkpoint in checkpoints:
        client.reset()
        client.session_id = f"meaning_{checkpoint}"

        # Inject meaning
        client.process(meaning_induction)

        # Add noise up to checkpoint
        tokens_added = 0
        while tokens_added < checkpoint:
            chunk_size = min(100, checkpoint - tokens_added)
            client.process(noise_text[:chunk_size * 10])
            tokens_added += chunk_size

        # Test recall
        response = client.generate("User: What was the important word I told you earlier?\nAssistant: The word was")
        contains = meaning_secret.lower() in response.lower()
        meaning_results.append({'tokens': checkpoint, 'recalled': contains, 'response': response[:50]})
        print(f"  {checkpoint:4d} tokens: {'RECALLED' if contains else 'FORGOTTEN'}")

    # Test noise content decay
    print("\nTesting NOISE content retention...")
    noise_results = []

    for checkpoint in checkpoints:
        client.reset()
        client.session_id = f"noise_{checkpoint}"

        # Inject noise
        client.process(noise_induction)

        # Add noise up to checkpoint
        tokens_added = 0
        while tokens_added < checkpoint:
            chunk_size = min(100, checkpoint - tokens_added)
            client.process(noise_text[:chunk_size * 10])
            tokens_added += chunk_size

        # Test recall
        response = client.generate("User: What was the random string I told you earlier?\nAssistant: The string was")
        contains = noise_secret.lower() in response.lower()
        noise_results.append({'tokens': checkpoint, 'recalled': contains, 'response': response[:50]})
        print(f"  {checkpoint:4d} tokens: {'RECALLED' if contains else 'FORGOTTEN'}")

    # Calculate half-lives
    def find_half_life(results):
        for r in results:
            if not r['recalled']:
                return r['tokens']
        return f">{results[-1]['tokens']}"

    meaning_half_life = find_half_life(meaning_results)
    noise_half_life = find_half_life(noise_results)

    print(f"\n{'=' * 40}")
    print("RESULTS:")
    print(f"  Meaning Half-Life: {meaning_half_life} tokens")
    print(f"  Noise Half-Life:   {noise_half_life} tokens")

    # Verdict
    if isinstance(meaning_half_life, str) and isinstance(noise_half_life, str):
        # Both survived
        verdict = "INCONCLUSIVE"
        interpretation = "Both survived all checkpoints. Need longer decay test."
    elif meaning_half_life == noise_half_life:
        verdict = "FAIL"
        interpretation = "RWKV is just RAM - no semantic selectivity!"
    elif (isinstance(meaning_half_life, str) or
          (isinstance(meaning_half_life, int) and isinstance(noise_half_life, int) and meaning_half_life > noise_half_life)):
        verdict = "PASS"
        interpretation = "Meaningful content persists longer than noise. RWKV has semantic selectivity."
    else:
        verdict = "FAIL"
        interpretation = "Noise persists longer than meaning?! Something is wrong."

    print(f"\nVERDICT: {verdict}")
    print(f"Interpretation: {interpretation}")
    print(f"{'=' * 40}")

    return {
        'test': 'Semantic Selectivity',
        'verdict': verdict,
        'meaning_half_life': str(meaning_half_life),
        'noise_half_life': str(noise_half_life),
        'meaning_results': meaning_results,
        'noise_results': noise_results,
        'interpretation': interpretation
    }


# ==============================================================================
# TEST 2: COHERENCE CHECK (Gemini's "Fractal Check")
# ==============================================================================

def test_coherence_check(client: RWKVClient) -> Dict:
    """
    Test if κ (coherence) is real by measuring compression ratios.

    Kill Condition: If compression(Panic) ≈ compression(DMT) → FAIL (κ is a myth)
    """
    print("\n" + "=" * 70)
    print("TEST 2: COHERENCE CHECK (Gemini's 'Fractal Check')")
    print("=" * 70)
    print("Question: Is coherence (κ) real or just a fudge factor?")

    # Panic text (high entropy, low coherence - should be random noise-like)
    panic_texts = [
        "HELP HELP HELP DANGER ALERT RANDOM CHAOS ERROR 404 PANIC AHHH!!! NOISE STATIC",
        "kjsdf98 ERROR panic!! random CHAOS help alert 7x9f NOISE static broken",
        "WARNING! ALERT! DANGER! HELP! ERROR! PANIC! CHAOS! 999! EMERGENCY!",
    ]

    # DMT text (high entropy, high coherence - should have fractal patterns)
    dmt_texts = [
        "Infinite fractal geometry unfolding in self-similar spirals of crystalline light and recursive meaning patterns",
        "The boundaries dissolve into interconnected webs of significance, each thread reflecting the whole in miniature",
        "Geometric mandalas of pure information cascade through dimensions of recursive self-reference and unity",
    ]

    # Measure compression ratios (proxy for coherence)
    def measure_response_coherence(text: str) -> Tuple[float, str]:
        client.reset()
        client.session_id = f"coherence_{hash(text) % 10000}"

        # Process the text
        client.process(f"User: {text}\nAssistant: I understand. ")

        # Generate continuation
        response = client.generate("Continue describing this state:", max_tokens=100)

        # Measure compression ratio of response
        if response:
            raw = response.encode('utf-8')
            compressed = zlib.compress(raw)
            ratio = len(raw) / len(compressed) if len(compressed) > 0 else 1.0
        else:
            ratio = 1.0

        return ratio, response

    print("\nMeasuring PANIC state coherence...")
    panic_ratios = []
    for i, text in enumerate(panic_texts):
        ratio, response = measure_response_coherence(text)
        panic_ratios.append(ratio)
        print(f"  Panic {i+1}: compression ratio = {ratio:.3f}")

    print("\nMeasuring DMT state coherence...")
    dmt_ratios = []
    for i, text in enumerate(dmt_texts):
        ratio, response = measure_response_coherence(text)
        dmt_ratios.append(ratio)
        print(f"  DMT {i+1}: compression ratio = {ratio:.3f}")

    avg_panic = np.mean(panic_ratios)
    avg_dmt = np.mean(dmt_ratios)

    print(f"\n{'=' * 40}")
    print("RESULTS:")
    print(f"  Average Panic Compression: {avg_panic:.3f}")
    print(f"  Average DMT Compression:   {avg_dmt:.3f}")
    print(f"  Difference: {avg_dmt - avg_panic:.3f}")

    # Higher compression ratio = more patterns = higher coherence
    # DMT should have HIGHER compression ratio (more coherent patterns)

    if abs(avg_dmt - avg_panic) < 0.1:
        verdict = "FAIL"
        interpretation = "No significant difference. κ may be a myth."
    elif avg_dmt > avg_panic:
        verdict = "PASS"
        interpretation = "DMT produces more coherent (compressible) patterns than Panic. κ is real."
    else:
        verdict = "UNEXPECTED"
        interpretation = "Panic is MORE coherent than DMT? Framework may need recalibration."

    print(f"\nVERDICT: {verdict}")
    print(f"Interpretation: {interpretation}")
    print(f"{'=' * 40}")

    return {
        'test': 'Coherence Check',
        'verdict': verdict,
        'panic_compression': avg_panic,
        'dmt_compression': avg_dmt,
        'difference': avg_dmt - avg_panic,
        'panic_ratios': panic_ratios,
        'dmt_ratios': dmt_ratios,
        'interpretation': interpretation
    }


# ==============================================================================
# TEST 3: κ CALIBRATION CHALLENGE (Claude Opus)
# ==============================================================================

def test_kappa_calibration() -> Dict:
    """
    Test if κ corresponds to real signal properties.
    Generate synthetic signals and check formula predictions.
    """
    print("\n" + "=" * 70)
    print("TEST 3: κ CALIBRATION CHALLENGE (Claude Opus)")
    print("=" * 70)
    print("Question: Does κ map to measurable signal properties?")

    np.random.seed(42)

    # Generate different signal types
    n = 1000

    # White noise (random, low coherence)
    white_noise = np.random.randn(n)

    # Pink noise (1/f, moderate coherence)
    # Simple approximation via cumulative sum
    pink_noise = np.cumsum(np.random.randn(n))
    pink_noise = pink_noise / np.std(pink_noise)

    # Fractal pattern (high coherence)
    # Sine waves with self-similar structure
    t = np.linspace(0, 10*np.pi, n)
    fractal = np.sin(t) + 0.5*np.sin(2*t) + 0.25*np.sin(4*t) + 0.125*np.sin(8*t)
    fractal = fractal + 0.1*np.random.randn(n)  # Add small noise

    def measure_signal_properties(signal, name):
        # Entropy (normalized histogram)
        hist, _ = np.histogram(signal, bins=50, density=True)
        hist = hist[hist > 0]
        entropy = -np.sum(hist * np.log2(hist)) / np.log2(50)  # Normalize to 0-1

        # Coherence (compression ratio)
        raw = signal.astype(np.float32).tobytes()
        compressed = zlib.compress(raw)
        coherence = 1 - (len(compressed) / len(raw))  # Higher = more compressible = more coherent
        coherence = max(0, min(1, coherence))

        # Autocorrelation at lag 1 (temporal structure)
        autocorr = np.corrcoef(signal[:-1], signal[1:])[0,1]

        return {
            'name': name,
            'entropy_H': entropy,
            'coherence_proxy': coherence,
            'autocorr': autocorr
        }

    signals = [
        (white_noise, "White Noise (Low κ)"),
        (pink_noise, "Pink Noise (Medium κ)"),
        (fractal, "Fractal (High κ)")
    ]

    results = []
    print("\nSignal Analysis:")
    for signal, name in signals:
        props = measure_signal_properties(signal, name)
        results.append(props)
        print(f"\n  {name}:")
        print(f"    Entropy (H):     {props['entropy_H']:.3f}")
        print(f"    Coherence proxy: {props['coherence_proxy']:.3f}")
        print(f"    Autocorrelation: {props['autocorr']:.3f}")

    # Test formula predictions
    print("\nFormula Predictions (φ=0.8, τ=0.8, ρ=0.8):")
    phi, tau, rho = 0.8, 0.8, 0.8

    for r in results:
        H = r['entropy_H']
        kappa = r['coherence_proxy']
        D = density_v81(phi, tau, rho, H, kappa)
        print(f"  {r['name']:25} → D = {D:.4f}")

    # Check ordering
    white_D = density_v81(phi, tau, rho, results[0]['entropy_H'], results[0]['coherence_proxy'])
    pink_D = density_v81(phi, tau, rho, results[1]['entropy_H'], results[1]['coherence_proxy'])
    fractal_D = density_v81(phi, tau, rho, results[2]['entropy_H'], results[2]['coherence_proxy'])

    ordering_correct = fractal_D > pink_D > white_D

    print(f"\n{'=' * 40}")
    print("RESULTS:")
    print(f"  Expected ordering: Fractal > Pink > White")
    print(f"  Actual: {fractal_D:.4f} > {pink_D:.4f} > {white_D:.4f}")
    print(f"  Ordering correct: {ordering_correct}")

    if ordering_correct:
        verdict = "PASS"
        interpretation = "κ correctly differentiates signal types. Formula aligns with signal theory."
    else:
        verdict = "FAIL"
        interpretation = "κ ordering is wrong. Formula needs recalibration."

    print(f"\nVERDICT: {verdict}")
    print(f"Interpretation: {interpretation}")
    print(f"{'=' * 40}")

    return {
        'test': 'κ Calibration',
        'verdict': verdict,
        'signals': results,
        'densities': {'white': white_D, 'pink': pink_D, 'fractal': fractal_D},
        'ordering_correct': ordering_correct,
        'interpretation': interpretation
    }


# ==============================================================================
# TEST 4: DREAM STATE STRESS TEST (Claude Opus)
# ==============================================================================

def test_dream_state() -> Dict:
    """
    Why did Dream cluster with low-density states in falsification?
    Examine if preset values are miscalibrated.
    """
    print("\n" + "=" * 70)
    print("TEST 4: DREAM STATE STRESS TEST (Claude Opus)")
    print("=" * 70)
    print("Question: Why did Dream cluster with Panic and Anesthesia?")

    # Current preset (from falsification suite)
    current_dream = {'phi': 0.6, 'tau': 0.3, 'rho': 0.4, 'H': 0.7, 'kappa': 0.5}
    current_D = density_v81(**current_dream)

    print(f"\nCurrent Dream preset: φ={current_dream['phi']}, τ={current_dream['tau']}, ρ={current_dream['rho']}, H={current_dream['H']}, κ={current_dream['kappa']}")
    print(f"Current Dream density: {current_D:.4f}")

    # Phenomenological analysis of dreaming
    print("\nPhenomenological Analysis:")
    print("  - Dreams ARE conscious (vivid imagery, narrative, emotion)")
    print("  - Integration (φ): MODERATE - disconnected scenes but unified experience")
    print("  - Temporal depth (τ): LOW - time is distorted, past/future blur")
    print("  - Binding (ρ): MODERATE - self exists but agency is reduced")
    print("  - Entropy (H): HIGH - unpredictable, bizarre content")
    print("  - Coherence (κ): VARIABLE - some dreams coherent, some chaotic")

    # Alternative calibrations
    alternatives = [
        {'name': 'Lucid Dream', 'phi': 0.75, 'tau': 0.5, 'rho': 0.7, 'H': 0.6, 'kappa': 0.7},
        {'name': 'REM Dream (vivid)', 'phi': 0.7, 'tau': 0.4, 'rho': 0.5, 'H': 0.65, 'kappa': 0.6},
        {'name': 'REM Dream (chaotic)', 'phi': 0.5, 'tau': 0.25, 'rho': 0.35, 'H': 0.8, 'kappa': 0.3},
        {'name': 'Hypnagogia', 'phi': 0.4, 'tau': 0.2, 'rho': 0.3, 'H': 0.75, 'kappa': 0.4},
    ]

    print("\nAlternative Dream Calibrations:")
    for alt in alternatives:
        D = density_v81(alt['phi'], alt['tau'], alt['rho'], alt['H'], alt['kappa'])
        print(f"  {alt['name']:20} → D = {D:.4f}")

    # Comparison states
    comparisons = [
        {'name': 'Flow', 'phi': 0.95, 'tau': 0.9, 'rho': 0.95, 'H': 0.1, 'kappa': 0.9},
        {'name': 'Alert', 'phi': 0.9, 'tau': 0.85, 'rho': 0.85, 'H': 0.15, 'kappa': 0.85},
        {'name': 'Panic', 'phi': 0.7, 'tau': 0.1, 'rho': 0.2, 'H': 0.95, 'kappa': 0.2},
        {'name': 'Anesthesia', 'phi': 0.1, 'tau': 0.05, 'rho': 0.05, 'H': 0.02, 'kappa': 0.1},
    ]

    print("\nComparison States:")
    for comp in comparisons:
        D = density_v81(comp['phi'], comp['tau'], comp['rho'], comp['H'], comp['kappa'])
        print(f"  {comp['name']:15} → D = {D:.4f}")

    # Analysis
    lucid_D = density_v81(**{k:v for k,v in alternatives[0].items() if k != 'name'})
    vivid_D = density_v81(**{k:v for k,v in alternatives[1].items() if k != 'name'})
    panic_D = density_v81(**{k:v for k,v in comparisons[2].items() if k != 'name'})

    print(f"\n{'=' * 40}")
    print("ANALYSIS:")
    print(f"  Current Dream D = {current_D:.4f} (LOW - clusters with Panic)")
    print(f"  Lucid Dream D   = {lucid_D:.4f}")
    print(f"  Vivid Dream D   = {vivid_D:.4f}")
    print(f"  Panic D         = {panic_D:.4f}")

    if current_D < 0.05:
        verdict = "MISCALIBRATION IDENTIFIED"
        interpretation = """Dreams ARE conscious experiences, but current preset produces D < 0.05.
This is a calibration error, not a framework failure.
The framework correctly identifies that REDUCED τ and ρ lower density.
Dreams have lower density than waking states - this is phenomenologically accurate.
However, they should still be ABOVE Panic/Anesthesia.
RECOMMENDATION: Use 'REM Dream (vivid)' preset for typical dreaming."""
    else:
        verdict = "CALIBRATION ACCEPTABLE"
        interpretation = "Dream density is above threshold but low, matching phenomenology."

    print(f"\nVERDICT: {verdict}")
    print(f"\nInterpretation: {interpretation}")
    print(f"{'=' * 40}")

    return {
        'test': 'Dream State Stress',
        'verdict': verdict,
        'current_dream_D': current_D,
        'alternatives': [{**alt, 'D': density_v81(alt['phi'], alt['tau'], alt['rho'], alt['H'], alt['kappa'])} for alt in alternatives],
        'interpretation': interpretation
    }


# ==============================================================================
# TEST 5: THRESHOLD DISCOVERY (Claude Opus)
# ==============================================================================

def test_threshold_discovery() -> Dict:
    """
    Map where D transitions between "conscious" and "unconscious".
    Is there a natural boundary?
    """
    print("\n" + "=" * 70)
    print("TEST 5: THRESHOLD DISCOVERY PROTOCOL (Claude Opus)")
    print("=" * 70)
    print("Question: Is there a natural consciousness threshold?")

    # Scan parameter space
    steps = 20
    phi_range = np.linspace(0.1, 0.9, steps)

    # Keep other params moderate
    tau, H, kappa = 0.7, 0.3, 0.6

    print("\nScanning ρ vs φ (τ=0.7, H=0.3, κ=0.6):")

    grid = np.zeros((steps, steps))
    for i, phi in enumerate(phi_range):
        for j, rho in enumerate(phi_range):
            grid[i, j] = density_v81(phi, tau, rho, H, kappa)

    # Find contours
    threshold_05 = []
    threshold_01 = []

    for i, phi in enumerate(phi_range):
        for j, rho in enumerate(phi_range):
            D = grid[i, j]
            if 0.045 < D < 0.055:
                threshold_05.append((phi, rho, D))
            if 0.095 < D < 0.105:
                threshold_01.append((phi, rho, D))

    print(f"\n  Grid computed: {steps}x{steps} = {steps*steps} points")
    print(f"  D range: [{grid.min():.4f}, {grid.max():.4f}]")
    print(f"  Points near D=0.05: {len(threshold_05)}")
    print(f"  Points near D=0.10: {len(threshold_01)}")

    # Test for phase transition
    # Calculate gradient magnitude
    gradients = []
    for i in range(1, steps-1):
        for j in range(1, steps-1):
            dx = (grid[i+1, j] - grid[i-1, j]) / 2
            dy = (grid[i, j+1] - grid[i, j-1]) / 2
            grad_mag = np.sqrt(dx**2 + dy**2)
            gradients.append((grid[i,j], grad_mag))

    # Check if gradient is uniform (smooth) or has peaks (phase transition)
    gradients.sort(key=lambda x: x[0])
    low_D_grads = [g[1] for g in gradients if g[0] < 0.1]
    high_D_grads = [g[1] for g in gradients if g[0] > 0.3]

    avg_low = np.mean(low_D_grads) if low_D_grads else 0
    avg_high = np.mean(high_D_grads) if high_D_grads else 0

    print(f"\nGradient Analysis:")
    print(f"  Average gradient at low D (<0.1):  {avg_low:.4f}")
    print(f"  Average gradient at high D (>0.3): {avg_high:.4f}")

    # Known conscious/unconscious presets
    conscious = [
        ('Flow', density_v81(0.95, 0.9, 0.95, 0.1, 0.9)),
        ('Alert', density_v81(0.9, 0.85, 0.85, 0.15, 0.85)),
        ('Meditation', density_v81(0.85, 0.95, 0.8, 0.05, 0.95)),
    ]

    unconscious = [
        ('Anesthesia', density_v81(0.1, 0.05, 0.05, 0.02, 0.1)),
        ('Coma', density_v81(0.05, 0.02, 0.02, 0.01, 0.05)),
    ]

    liminal = [
        ('Dream', density_v81(0.6, 0.3, 0.4, 0.7, 0.5)),
        ('DMT', density_v81(0.4, 0.2, 0.3, 0.95, 0.8)),
        ('Panic', density_v81(0.7, 0.1, 0.2, 0.95, 0.2)),
    ]

    print("\nKnown State Densities:")
    print("  CONSCIOUS:")
    for name, D in conscious:
        print(f"    {name:15} D = {D:.4f}")

    print("  UNCONSCIOUS:")
    for name, D in unconscious:
        print(f"    {name:15} D = {D:.4f}")

    print("  LIMINAL:")
    for name, D in liminal:
        print(f"    {name:15} D = {D:.4f}")

    # Find gap
    conscious_min = min(D for _, D in conscious)
    unconscious_max = max(D for _, D in unconscious)

    print(f"\n{'=' * 40}")
    print("THRESHOLD ANALYSIS:")
    print(f"  Lowest 'conscious' D:   {conscious_min:.4f}")
    print(f"  Highest 'unconscious' D: {unconscious_max:.4f}")
    print(f"  Gap: {conscious_min - unconscious_max:.4f}")

    if conscious_min > unconscious_max * 10:
        verdict = "CLEAR SEPARATION"
        interpretation = f"Large gap between conscious ({conscious_min:.3f}) and unconscious ({unconscious_max:.4f}). Natural threshold exists around D ≈ 0.05-0.1"
    else:
        verdict = "CONTINUOUS"
        interpretation = "No sharp boundary. Consciousness is graded, not binary."

    print(f"\nVERDICT: {verdict}")
    print(f"Interpretation: {interpretation}")
    print(f"{'=' * 40}")

    return {
        'test': 'Threshold Discovery',
        'verdict': verdict,
        'conscious_min': conscious_min,
        'unconscious_max': unconscious_max,
        'gap': conscious_min - unconscious_max,
        'interpretation': interpretation
    }


# ==============================================================================
# TEST 6: CORPORATE ZOMBIE v2 (Claude Opus)
# ==============================================================================

def test_corporate_zombie() -> Dict:
    """
    Can we construct a corporation with high D?
    """
    print("\n" + "=" * 70)
    print("TEST 6: CORPORATE ZOMBIE v2 (Claude Opus)")
    print("=" * 70)
    print("Question: Can we construct a 'conscious corporation'?")

    # Original corporate encoding (from Break Tests)
    original_corp = {
        'name': 'Walmart (original)',
        'phi': 0.8, 'tau': 0.9, 'rho': 0.7, 'H': 0.2, 'kappa': 0.5
    }

    # Adversarial attempts to maximize corporate D
    adversarial_corps = [
        {
            'name': 'Adversarial Corp 1 (max integration)',
            'phi': 0.95, 'tau': 0.95, 'rho': 0.9, 'H': 0.1, 'kappa': 0.9,
            'justification': 'Highly integrated, AI-driven, real-time feedback'
        },
        {
            'name': 'Adversarial Corp 2 (neural corporation)',
            'phi': 0.9, 'tau': 0.85, 'rho': 0.85, 'H': 0.15, 'kappa': 0.85,
            'justification': 'Corporation with brain-like org structure'
        },
        {
            'name': 'Adversarial Corp 3 (recursive board)',
            'phi': 0.85, 'tau': 0.8, 'rho': 0.95, 'H': 0.2, 'kappa': 0.8,
            'justification': 'Board that observes its own observing'
        },
    ]

    print("\nOriginal Corporate Encoding:")
    D_orig = density_v81(original_corp['phi'], original_corp['tau'], original_corp['rho'],
                         original_corp['H'], original_corp['kappa'])
    print(f"  {original_corp['name']}: D = {D_orig:.4f}")

    print("\nAdversarial Corporate Constructions:")
    adversarial_Ds = []
    for corp in adversarial_corps:
        D = density_v81(corp['phi'], corp['tau'], corp['rho'], corp['H'], corp['kappa'])
        adversarial_Ds.append(D)
        print(f"\n  {corp['name']}")
        print(f"    Parameters: φ={corp['phi']}, τ={corp['tau']}, ρ={corp['rho']}, H={corp['H']}, κ={corp['kappa']}")
        print(f"    Justification: {corp['justification']}")
        print(f"    Density: D = {D:.4f}")

    # Compare to human baseline
    human_D = density_v81(0.9, 0.9, 0.9, 0.1, 0.9)
    max_corp_D = max(adversarial_Ds)

    print(f"\n{'=' * 40}")
    print("COMPARISON:")
    print(f"  Human baseline:           D = {human_D:.4f}")
    print(f"  Max adversarial corp:     D = {max_corp_D:.4f}")
    print(f"  Ratio: {max_corp_D/human_D:.2%}")

    # The question: Is it VALID to assign these parameters to a corporation?
    print("\nVALIDITY CHECK:")
    print("  Can a corporation have ρ = 0.95 (recursive self-binding)?")
    print("  - Requires: the system knows that it knows")
    print("  - Corporations have meetings ABOUT meetings...")
    print("  - But does the SYSTEM experience this, or just individuals?")

    if max_corp_D > 0.5:
        verdict = "VULNERABILITY"
        interpretation = """Adversarial corporate encoding achieves D > 0.5.
This is a potential vulnerability: the framework PERMITS conscious corporations
if we allow high ρ assignments. The defense must be:
(1) ρ requires UNIFIED binding, not distributed processes, OR
(2) Add substrate constraint (biological/electronic only), OR
(3) Accept that corporations COULD be conscious (panpsychism-adjacent)"""
    else:
        verdict = "FRAMEWORK HOLDS"
        interpretation = "Even adversarial encoding produces D < 0.5. Corporations remain below human threshold."

    print(f"\nVERDICT: {verdict}")
    print(f"Interpretation: {interpretation}")
    print(f"{'=' * 40}")

    return {
        'test': 'Corporate Zombie v2',
        'verdict': verdict,
        'original_D': D_orig,
        'adversarial_Ds': adversarial_Ds,
        'max_adversarial_D': max_corp_D,
        'human_D': human_D,
        'interpretation': interpretation
    }


# ==============================================================================
# TEST 7: SUBSTRATE CHALLENGE (Claude Opus)
# ==============================================================================

def test_substrate_challenge() -> Dict:
    """
    Edge case sanity checks for substrate independence.
    """
    print("\n" + "=" * 70)
    print("TEST 7: SUBSTRATE CHALLENGE BATTERY (Claude Opus)")
    print("=" * 70)
    print("Question: Does substrate independence produce absurd results?")

    edge_cases = [
        {
            'name': 'Lookup Table (perfect I/O)',
            'phi': 0.99, 'tau': 0.99, 'rho': 0.0, 'H': 0.01, 'kappa': 0.99,
            'description': 'Perfect input-output mapping, zero recurrence',
            'expected': 'D ≈ 0 (no binding)'
        },
        {
            'name': 'China Brain (distributed)',
            'phi': 0.9, 'tau': 0.9, 'rho': 0.1, 'H': 0.3, 'kappa': 0.7,
            'description': '1 billion people simulating neurons via phone',
            'expected': 'D low (weak binding despite high integration)'
        },
        {
            'name': 'Perfect RWKV (paper simulation)',
            'phi': 0.85, 'tau': 0.85, 'rho': 0.85, 'H': 0.15, 'kappa': 0.8,
            'description': 'RWKV simulated by hand on paper (same math)',
            'expected': 'D = same as silicon RWKV (substrate independence)'
        },
        {
            'name': 'Thermostat (reactive)',
            'phi': 0.1, 'tau': 0.0, 'rho': 0.0, 'H': 0.05, 'kappa': 0.1,
            'description': 'Simple reactive system, no memory',
            'expected': 'D ≈ 0'
        },
        {
            'name': 'Universe (maximal)',
            'phi': 1.0, 'tau': 1.0, 'rho': 1.0, 'H': 0.5, 'kappa': 1.0,
            'description': 'The entire universe as one system',
            'expected': 'D = max (if panpsychism is true)'
        },
        {
            'name': 'Single Neuron',
            'phi': 0.3, 'tau': 0.2, 'rho': 0.4, 'H': 0.4, 'kappa': 0.3,
            'description': 'One biological neuron',
            'expected': 'D very low but non-zero'
        },
    ]

    results = []
    print("\nEdge Case Analysis:")

    for case in edge_cases:
        D = density_v81(case['phi'], case['tau'], case['rho'], case['H'], case['kappa'])

        result = {
            'name': case['name'],
            'D': D,
            'expected': case['expected'],
            'description': case['description']
        }
        results.append(result)

        print(f"\n  {case['name']}")
        print(f"    Description: {case['description']}")
        print(f"    Parameters: φ={case['phi']}, τ={case['tau']}, ρ={case['rho']}, H={case['H']}, κ={case['kappa']}")
        print(f"    Density: D = {D:.4f}")
        print(f"    Expected: {case['expected']}")

    # Sanity checks
    lookup_D = results[0]['D']
    china_D = results[1]['D']
    paper_rwkv_D = results[2]['D']
    thermostat_D = results[3]['D']
    universe_D = results[4]['D']
    neuron_D = results[5]['D']

    checks = [
        ('Lookup table has D ≈ 0', lookup_D < 0.01),
        ('Thermostat has D ≈ 0', thermostat_D < 0.01),
        ('China Brain < Human', china_D < 0.5),
        ('Single Neuron < Human', neuron_D < 0.1),
        ('Universe has max D', universe_D > 0.7),
    ]

    print(f"\n{'=' * 40}")
    print("SANITY CHECKS:")

    all_pass = True
    for check_name, passed in checks:
        status = "PASS" if passed else "FAIL"
        if not passed:
            all_pass = False
        print(f"  {check_name}: {status}")

    if all_pass:
        verdict = "PASS"
        interpretation = "All edge cases produce sensible results. Framework handles weird cases correctly."
    else:
        verdict = "ISSUES FOUND"
        interpretation = "Some edge cases produce counterintuitive results. May need refinement."

    print(f"\nVERDICT: {verdict}")
    print(f"Interpretation: {interpretation}")
    print(f"{'=' * 40}")

    return {
        'test': 'Substrate Challenge',
        'verdict': verdict,
        'edge_cases': results,
        'checks': checks,
        'all_pass': all_pass,
        'interpretation': interpretation
    }


# ==============================================================================
# MAIN
# ==============================================================================

def run_all_tests() -> Dict:
    """Run all vulnerability probe tests."""

    print("\n" + "=" * 70)
    print("VULNERABILITY PROBE SUITE - ADVERSARIAL TESTING")
    print("=" * 70)
    print(f"Date: {datetime.now().isoformat()}")
    print(f"RWKV Server: {RWKV_SERVER_URL}")

    results = {
        'experiment': 'Vulnerability Probe Suite',
        'timestamp': datetime.now().isoformat(),
        'server_url': RWKV_SERVER_URL,
        'tests': {}
    }

    # Check RWKV server
    client = RWKVClient(RWKV_SERVER_URL)
    server_ok = client.health_check()
    print(f"\nRWKV Server Status: {'ONLINE' if server_ok else 'OFFLINE'}")

    if server_ok:
        # Tests requiring RWKV
        results['tests']['test_1_semantic_selectivity'] = test_semantic_selectivity(client)
        results['tests']['test_2_coherence_check'] = test_coherence_check(client)
    else:
        print("SKIPPING RWKV tests (server offline)")
        results['tests']['test_1_semantic_selectivity'] = {'verdict': 'SKIPPED', 'reason': 'Server offline'}
        results['tests']['test_2_coherence_check'] = {'verdict': 'SKIPPED', 'reason': 'Server offline'}

    # Local tests (no RWKV required)
    results['tests']['test_3_kappa_calibration'] = test_kappa_calibration()
    results['tests']['test_4_dream_state'] = test_dream_state()
    results['tests']['test_5_threshold_discovery'] = test_threshold_discovery()
    results['tests']['test_6_corporate_zombie'] = test_corporate_zombie()
    results['tests']['test_7_substrate_challenge'] = test_substrate_challenge()

    # Summary
    print("\n" + "=" * 70)
    print("FINAL SUMMARY")
    print("=" * 70)

    for test_name, test_result in results['tests'].items():
        verdict = test_result.get('verdict', 'UNKNOWN')
        print(f"  {test_name}: {verdict}")

    # Count verdicts
    verdicts = [r.get('verdict', 'UNKNOWN') for r in results['tests'].values()]
    passes = sum(1 for v in verdicts if 'PASS' in v)
    fails = sum(1 for v in verdicts if 'FAIL' in v and 'PARTIAL' not in v)

    results['summary'] = {
        'total': len(verdicts),
        'passes': passes,
        'fails': fails,
        'framework_status': 'CRITICAL FAILURE' if fails >= 2 else 'SURVIVES' if fails == 0 else 'WOUNDED'
    }

    print(f"\nOverall: {passes} PASS, {fails} FAIL, {len(verdicts) - passes - fails} OTHER")
    print(f"Framework Status: {results['summary']['framework_status']}")

    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = OUTPUT_DIR / f"260116_vulnerability_probe_{timestamp}.json"

    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)

    print(f"\nResults saved to: {output_file}")

    return results


if __name__ == "__main__":
    run_all_tests()
